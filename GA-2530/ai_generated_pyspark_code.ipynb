{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c904c6d-5afe-44fc-aad9-f7a078e3e0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Cardinal Health Data\n",
    "# MAGIC This notebook performs an ETL process on data from Unity Catalog tables, including data integration, custom calculations, filtering, aggregation, and writing the output back to Unity Catalog.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, broadcast\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Assume the Spark session is already available as 'spark'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data from Unity Catalog tables\n",
    "        logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "        associates_df = spark.table(\"genai_demo.cardinal_health.associates_employment\")\n",
    "        compensation_df = spark.table(\"genai_demo.cardinal_health.compensation_guidelines\")\n",
    "        growth_df = spark.table(\"genai_demo.cardinal_health.growth_opportunities\")\n",
    "        hospital_assignments_df = spark.table(\"genai_demo.cardinal_health.hospital_assignments\")\n",
    "        hospitals_stats_df = spark.table(\"genai_demo.cardinal_health.hospitals_stats\")\n",
    "        logistics_channels_df = spark.table(\"genai_demo.cardinal_health.logistics_channels\")\n",
    "\n",
    "        # Data Integration: Join operations\n",
    "        logger.info(\"Performing join operations...\")\n",
    "        # Join associates with compensation\n",
    "        joined_df = associates_df.join(broadcast(compensation_df), \"Associate_ID\", \"inner\")\n",
    "\n",
    "        # Join hospital assignments with hospitals stats\n",
    "        hospital_joined_df = hospital_assignments_df.join(hospitals_stats_df, \"Hospital_ID\", \"inner\")\n",
    "\n",
    "        # Join logistics channels with growth opportunities\n",
    "        logistics_joined_df = logistics_channels_df.join(broadcast(growth_df), [\"Channel_ID\", \"Channel_Type\"], \"inner\")\n",
    "\n",
    "        # Join all together\n",
    "        final_joined_df = joined_df.join(hospital_joined_df, \"Associate_ID\", \"inner\") \\\n",
    "                                   .join(logistics_joined_df, \"Hospital_ID\", \"inner\")\n",
    "\n",
    "        # Custom Calculations\n",
    "        logger.info(\"Calculating total compensation and projected revenue...\")\n",
    "        final_joined_df = final_joined_df.withColumn(\"Total_Compensation\", \n",
    "                                                     col(\"Base_Salary\") + \n",
    "                                                     (col(\"Commission_Percentage\") / 100) * col(\"Base_Salary\") + \n",
    "                                                     col(\"Bonus\"))\n",
    "\n",
    "        final_joined_df = final_joined_df.withColumn(\"Projected_Revenue\", \n",
    "                                                     expr(\"Sales_Revenue * (1 + Projected_Sales_Growth_Rate / 100)\"))\n",
    "\n",
    "        # Data Filtering and Selection\n",
    "        logger.info(\"Filtering data for target years beyond 2023...\")\n",
    "        filtered_df = final_joined_df.filter(col(\"Target_Year\") > 2023) \\\n",
    "                                     .select(\"Hospital_ID\", \"Associate_ID\", \"Total_Compensation\", \"Projected_Revenue\")\n",
    "\n",
    "        # Aggregation and Business Logic\n",
    "        logger.info(\"Aggregating data...\")\n",
    "        aggregated_df = filtered_df.groupBy(\"Hospital_ID\").agg({\"Total_Compensation\": \"sum\", \"Projected_Revenue\": \"sum\"})\n",
    "\n",
    "        # Output Data Configuration: Write to Unity Catalog\n",
    "        logger.info(\"Writing the processed data to Unity Catalog...\")\n",
    "        aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.cardinal_health.final_output\")\n",
    "\n",
    "        logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"An error occurred during the ETL process\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
