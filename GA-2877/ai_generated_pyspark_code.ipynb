{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# MAGIC %md\n# MAGIC # Customer 360 Data Processing\n# MAGIC This notebook processes customer data from various sources to create a comprehensive Customer 360 view. The data is loaded, transformed, and written to Unity Catalog.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Initialize logging\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already available as 'spark'\n\n# COMMAND ----------\n# MAGIC\n",
                "# Load data from Unity Catalog tables\ntry:\n    policy_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/policycsv\", header=True, inferSchema=True)\n    claims_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/claimscsv\", header=True, inferSchema=True)\n    demographics_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/demographicscsv\", header=True, inferSchema=True)\n    scores_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/scorescsv\", header=True, inferSchema=True)\n    aiml_insights_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/aiml_insightscsv\", header=True, inferSchema=True)\n    logger.info(\"Data loaded successfully from CSV files.\")\nexcept Exception as e:\n    logger.error(f\"Error loading data: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Select required fields\ndemographics_df = demographics_df.select(\n    \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\", \n    \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n)\nclaims_df = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\")\npolicy_df = policy_df.select(\"policy_id\", \"customer_id\", \"policy_type\", \"policy_status\", \"policy_start_date\", \"policy_end_date\", \"policy_term\", \"policy_premium\", \"total_premium_paid\", \"renewal_status\", \"policy_addons\")\nscores_df = scores_df.select(\"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\")\naiml_insights_df = aiml_insights_df.select(\"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \"Claims_Fraud_Probability\", \"Revenue_Potential\")\n\n# COMMAND ----------\n# MAGIC\n",
                "# Perform joins\ntry:\n    joined_df = demographics_df.join(policy_df, F.col('Customer_ID') == F.col('customer_id'), \"inner\") \\\n                               .join(claims_df, F.col('policy_id') == F.col('Policy_ID'), \"inner\")\n    logger.info(\"Data joined successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during join operations: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Aggregate claims data\ntry:\n    summarized_df = claims_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.count(\"Policy_ID\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n    logger.info(\"Claims data aggregated successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during aggregation: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Join summarized data\ntry:\n    final_df = joined_df.join(summarized_df, \"Customer_ID\", \"inner\")\n    logger.info(\"Summarized data joined successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during final join: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Calculate custom metrics\ntry:\n    # Define complex conditions separately for readability\n    claim_to_premium_ratio = F.when(F.col(\"total_premium_paid\") > 0, F.col(\"Claim_Amount\") / F.col(\"total_premium_paid\")).otherwise(0)\n    claims_per_policy = F.when(F.col(\"Policy_Count\") > 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0)\n\n    final_df = final_df.withColumn(\"Age\", F.datediff(F.current_date(), F.col(\"Date_of_Birth\")) / 365) \\\n                       .withColumn(\"Claim_To_Premium_Ratio\", claim_to_premium_ratio) \\\n                       .withColumn(\"Claims_Per_Policy\", claims_per_policy) \\\n                       .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n                       .withColumn(\"Cross_Sell_Opportunities\", F.lit(\"['MultiPolicy Discount', 'Home Coverage Addon']\")) \\\n                       .withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n    logger.info(\"Custom metrics calculated successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during custom metric calculations: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Combine all data\ntry:\n    customer_360_df = final_df.join(aiml_insights_df, \"Customer_ID\", \"inner\") \\\n                              .join(scores_df, \"Customer_ID\", \"inner\")\n    logger.info(\"All data combined successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data combination: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Write output to Unity Catalog\ntry:\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.jnj.Customer_360\")\n    customer_360_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.jnj.Customer_360\")\n    logger.info(\"Data written to Unity Catalog successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}