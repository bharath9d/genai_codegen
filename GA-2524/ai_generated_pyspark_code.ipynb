{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05c827b-d3ec-492c-a9d6-b8fbef438fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Customer 360 ETL Process\n",
    "# MAGIC This notebook performs an ETL process to create a comprehensive view of customer data by integrating various data sources.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    # Step 1: Data Source Configuration\n",
    "    logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "    claims_df = spark.table(\"genai_demo.guardian.claims\")\n",
    "    demographics_df = spark.table(\"genai_demo.guardian.demographics\")\n",
    "    policy_df = spark.table(\"genai_demo.guardian.policy\")\n",
    "    scores_df = spark.table(\"genai_demo.guardian.scores\")\n",
    "    aiml_insights_df = spark.table(\"genai_demo.guardian.aiml_insights\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 2: Data Selection and Filtering\n",
    "    logger.info(\"Selecting relevant fields from demographics and claims data...\")\n",
    "    selected_demographics_df = demographics_df.select(\n",
    "        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\",\n",
    "        \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n",
    "    )\n",
    "    selected_claims_df = claims_df.select(\n",
    "        \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 3: Data Integration\n",
    "    logger.info(\"Joining demographics and policy data on Customer_ID...\")\n",
    "    joined_df = selected_demographics_df.join(policy_df, \"Customer_ID\").join(selected_claims_df, \"Policy_ID\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 4: Data Aggregation\n",
    "    logger.info(\"Aggregating claims data to calculate metrics...\")\n",
    "    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n",
    "        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n",
    "        F.countDistinct(\"Policy_ID\").alias(\"Policy_Count\"),\n",
    "        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n",
    "        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n",
    "        F.first(\"Date_of_Birth\").alias(\"Date_of_Birth\"),  # Ensure Date_of_Birth is available for Age calculation\n",
    "        F.first(\"Total_Premium_Paid\").alias(\"Total_Premium_Paid\")  # Ensure Total_Premium_Paid is available\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 5: Custom Calculations\n",
    "    logger.info(\"Performing custom calculations...\")\n",
    "    final_df = aggregated_df.withColumn(\"Age\", F.datediff(F.current_date(), F.col(\"Date_of_Birth\")) / 365) \\\n",
    "        .withColumn(\"Claim_To_Premium_Ratio\", F.when(F.col(\"Total_Premium_Paid\") != 0, F.col(\"Average_Claim_Amount\") / F.col(\"Total_Premium_Paid\")).otherwise(0)) \\\n",
    "        .withColumn(\"Claims_Per_Policy\", F.when(F.col(\"Policy_Count\") != 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0)) \\\n",
    "        .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n",
    "        .withColumn(\"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n",
    "        .withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 6: Comprehensive Data Joining\n",
    "    logger.info(\"Integrating data from all sources...\")\n",
    "    customer_360_df = final_df.join(scores_df, \"Customer_ID\").join(aiml_insights_df, \"Customer_ID\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 7: Output Data\n",
    "    logger.info(\"Writing the consolidated view of customer data to Delta format...\")\n",
    "    customer_360_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/dataeconomy-8ixr/62446/output/Customer_360\")\n",
    "\n",
    "    logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during the ETL process: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365d343b-b5cf-4e8b-863d-e5864d6d57dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Customer 360 Profile\n",
    "# MAGIC This notebook performs an ETL process to create a comprehensive customer profile by integrating data from various sources.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    # Load data from Unity Catalog tables\n",
    "    logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n",
    "    claims_df = spark.table(\"genai_demo.jnj.claims\")\n",
    "    policy_df = spark.table(\"genai_demo.jnj.policy\")\n",
    "    scores_df = spark.table(\"genai_demo.jnj.scores\")\n",
    "    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 1: Data Selection and Filtering\n",
    "    logger.info(\"Selecting relevant fields from each dataset...\")\n",
    "    selected_demographics_df = demographics_df.select(\"Customer_ID\", \"Customer_Name\", \"Email\", \"Date_of_Birth\")\n",
    "    selected_claims_df = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Amount\")\n",
    "    #selected_policy_df = policy_df.select(\"policy_id\", \"customer_id\", \"policy_type\", \"total_premium_paid\")\n",
    "    selected_policy_df = policy_df.select(\n",
    "        F.col(\"policy_id\").alias(\"policy_policy_id\"), \n",
    "        F.col(\"customer_id\").alias(\"policy_customer_id\"), \n",
    "        F.col(\"policy_type\"), \n",
    "        F.col(\"total_premium_paid\")\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 2: Data Integration\n",
    "    logger.info(\"Joining datasets based on common identifiers...\")\n",
    "    joined_df = selected_demographics_df.join(selected_policy_df, selected_demographics_df.Customer_ID == selected_policy_df.policy_customer_id, \"inner\").drop(\"policy_customer_id\")\n",
    "    joined_df = joined_df.join(selected_claims_df, joined_df.policy_policy_id == selected_claims_df.Policy_ID, \"inner\").drop(\"policy_policy_id\", \"Policy_ID\")\n",
    "\n",
    "    # Cache the joined DataFrame for performance\n",
    "    joined_df.cache()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 3: Data Aggregation\n",
    "    logger.info(\"Aggregating data to compute metrics...\")\n",
    "    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n",
    "        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n",
    "        F.count(\"policy_id\").alias(\"Policy_Count\"),\n",
    "        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n",
    "        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 4: Custom Calculations\n",
    "    logger.info(\"Performing custom calculations...\")\n",
    "    final_df = aggregated_df.withColumn(\"Age\", F.datediff(F.current_date(), F.to_date(\"Date_of_Birth\", \"yyyy-MM-dd\")) / 365) \\\n",
    "        .withColumn(\"Claim_To_Premium_Ratio\", F.when(aggregated_df.total_premium_paid != 0, aggregated_df.Average_Claim_Amount / aggregated_df.total_premium_paid).otherwise(0)) \\\n",
    "        .withColumn(\"Claims_Per_Policy\", F.when(aggregated_df.Policy_Count != 0, aggregated_df.Total_Claims / aggregated_df.Policy_Count).otherwise(0))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Step 5: Comprehensive Data Joining\n",
    "    logger.info(\"Integrating all data sources into a comprehensive customer profile...\")\n",
    "    comprehensive_profile_df = final_df.join(aiml_insights_df, \"Customer_ID\", \"inner\") \\\n",
    "        .join(scores_df, \"Customer_ID\", \"inner\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Output Configuration\n",
    "    logger.info(\"Writing the comprehensive customer profile to Unity Catalog...\")\n",
    "    comprehensive_profile_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.jnj.customer_360_profile\")\n",
    "\n",
    "    logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
