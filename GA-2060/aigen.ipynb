{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process for Superstore Sales Data\n# MAGIC This notebook performs an ETL process on Superstore sales data using PySpark. It loads data from Unity Catalog tables, performs transformations, and saves the results back to the catalog.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, StringType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\ntry:\n    # Load Orders data from Unity Catalog tables\n    orders_central_df = spark.table(\"catalog.db.orders_central\")\n    orders_west_df = spark.table(\"catalog.db.orders_west\")\n    orders_east_df = spark.table(\"catalog.db.orders_east\")\n    orders_south_df = spark.table(\"catalog.db.orders_south\")\n\n    # Load Quota data\n    quota_df = spark.table(\"catalog.db.quota\")\n\n    # Load Returns data\n    returns_df = spark.table(\"catalog.db.returns\")\n\n    logger.info(\"Data loaded successfully from Unity Catalog tables.\")\n\n# COMMAND ----------\n\n    # Standardize column names\n    orders_central_df = orders_central_df.withColumnRenamed(\"Discounts\", \"Discount\").withColumnRenamed(\"Product\", \"Product Name\")\n\n    # Exclude null entries\n    orders_central_df = orders_central_df.filter(orders_central_df[\"Order ID\"].isNotNull())\n\n# COMMAND ----------\n\n    # Union datasets\n    all_orders_df = orders_central_df.union(orders_west_df).union(orders_east_df).union(orders_south_df).cache()\n\n# COMMAND ----------\n\n    # Join with returns\n    returns_df = returns_df.drop(\"Order Date\", \"Sub-Category\")\n    enriched_orders_df = all_orders_df.join(F.broadcast(returns_df), [\"Order ID\", \"Product ID\"], \"left\").withColumn(\"Returned?\", returns_df[\"Return Reason\"].isNotNull()).cache()\n\n# COMMAND ----------\n\n    # Calculate Days to Ship\n    enriched_orders_df = enriched_orders_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n\n    # Extract Year of Sale\n    enriched_orders_df = enriched_orders_df.withColumn(\"Year of Sale\", F.year(F.col(\"Order Date\")))\n\n# COMMAND ----------\n\n    # Apply discount filter\n    filtered_orders_df = enriched_orders_df.filter(~((F.col(\"Discount\") >= 17) & (F.col(\"Discount\") <= 18)))\n\n# COMMAND ----------\n\n    # Aggregate sales data\n    aggregated_sales_df = filtered_orders_df.groupBy(\"Region\", \"Year of Sale\").agg(\n        F.sum(\"Profit\").alias(\"Total Profit\"),\n        F.sum(\"Sales\").alias(\"Total Sales\"),\n        F.sum(\"Quantity\").alias(\"Total Quantity\"),\n        F.avg(\"Discount\").alias(\"Average Discount\")\n    )\n\n    logger.info(\"Data transformation completed successfully.\")\n\n# COMMAND ----------\n\n    # Save to Databricks catalog\n    aggregated_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.annual_regional_performance\")\n    filtered_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.superstore_sales\")\n\n    logger.info(\"Data saved successfully to Unity Catalog tables.\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}