{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# Import necessary libraries\nimport pyspark.sql.functions as F\nimport logging\nfrom pyspark.sql.functions import broadcast\n\n# COMMAND ----------\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\ntry:\n    # Step 1: Data Source Configuration\n    logger.info(\"Loading data from CSV files into DataFrames.\")\n    policy_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/policy.csv\", header=True, inferSchema=True)\n    claims_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/claims.csv\", header=True, inferSchema=True)\n    demographics_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/demographics.csv\", header=True, inferSchema=True)\n    scores_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/scores.csv\", header=True, inferSchema=True)\n    aiml_insights_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/aiml_insights.csv\", header=True, inferSchema=True)\n\n# COMMAND ----------\n    # Step 2: Data Selection and Filtering\n    logger.info(\"Selecting relevant fields from demographics data.\")\n    selected_demographics_df = demographics_df.select(\n        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\", \n        \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n    )\n\n# COMMAND ----------\n    # Step 3: Data Integration\n    logger.info(\"Integrating demographics and policy data.\")\n    integrated_df = selected_demographics_df.join(policy_df, selected_demographics_df.Customer_ID == policy_df.customer_id, \"inner\")\n    integrated_df = integrated_df.drop(policy_df.customer_id)  # Remove duplicate columns\n\n    logger.info(\"Integrating claims data.\")\n    integrated_df = integrated_df.join(claims_df, integrated_df.policy_id == claims_df.Policy_ID, \"inner\")\n    integrated_df = integrated_df.drop(claims_df.Policy_ID)  # Remove duplicate columns\n\n# COMMAND ----------\n    # Step 4: Data Aggregation\n    logger.info(\"Aggregating claims data.\")\n    aggregated_df = claims_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.countDistinct(\"policy_id\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n\n# COMMAND ----------\n    # Step 5: Custom Calculations\n    logger.info(\"Performing custom calculations.\")\n    final_df = integrated_df.withColumn(\"Age\", F.datediff(F.current_date(), F.to_date(\"Date_of_Birth\", \"yyyy-MM-dd\")) / 365)\n    final_df = final_df.withColumn(\"Claim_To_Premium_Ratio\", F.when(F.col(\"total_premium_paid\") != 0, F.col(\"Claim_Amount\") / F.col(\"total_premium_paid\")).otherwise(0))\n    final_df = final_df.withColumn(\"Claims_Per_Policy\", F.when(F.col(\"Policy_Count\") != 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0))\n    final_df = final_df.withColumn(\"Retention_Rate\", F.lit(0.85))\n    final_df = final_df.withColumn(\"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\"))\n    final_df = final_df.withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n\n# COMMAND ----------\n    # Step 6: Comprehensive Data Consolidation\n    logger.info(\"Consolidating all data sources for a complete customer profile.\")\n    customer_360_df = final_df.join(broadcast(scores_df), \"Customer_ID\", \"inner\").join(broadcast(aiml_insights_df), \"Customer_ID\", \"inner\")\n\n# COMMAND ----------\n    # Step 7: Output Generation\n    logger.info(\"Writing the final Customer 360 View to a Unity Catalog table.\")\n    customer_360_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360_view\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process: %s\", e)\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}