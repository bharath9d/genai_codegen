{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process for Customer 360 Data\n# MAGIC This notebook performs an ETL process to create a comprehensive Customer 360 dataset using data from various Unity Catalog tables.\n\n# COMMAND ----------\n\nimport pyspark.sql.functions as F\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 1: Data Source Configuration\n# MAGIC Load data from Unity Catalog tables.\n\n# COMMAND ----------\n\ntry:\n    logger.info(\"Loading data from Unity Catalog tables\")\n    policy_df = spark.table(\"catalog.source_db.policy\")\n    claims_df = spark.table(\"catalog.source_db.claims\")\n    demographics_df = spark.table(\"catalog.source_db.demographics\")\n    scores_df = spark.table(\"catalog.source_db.scores\")\n    aiml_insights_df = spark.table(\"catalog.source_db.aiml_insights\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 2: Data Selection and Filtering\n# MAGIC Select relevant fields from each data source.\n\n# COMMAND ----------\n\n    logger.info(\"Selecting relevant fields from each data source\")\n    demographics_selected = demographics_df.select(\n        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\",\n        \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n    )\n    claims_selected = claims_df.select(\n        \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n    )\n    policy_selected = policy_df.select(\n        \"policy_id\", \"customer_id\", \"policy_type\", \"policy_status\", \"policy_start_date\", \"policy_end_date\",\n        \"policy_term\", \"policy_premium\", \"total_premium_paid\", \"renewal_status\", \"policy_addons\"\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 3: Data Integration\n# MAGIC Join datasets based on key identifiers.\n\n# COMMAND ----------\n\n    logger.info(\"Joining datasets based on key identifiers\")\n    joined_df = demographics_selected.join(\n        policy_selected, demographics_selected.Customer_ID == policy_selected.customer_id, \"inner\"\n    ).join(\n        claims_selected, policy_selected.policy_id == claims_selected.Policy_ID, \"inner\"\n    )\n\n    # Cache the joined DataFrame if reused multiple times\n    joined_df.cache()\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 4: Data Aggregation\n# MAGIC Compute aggregate metrics.\n\n# COMMAND ----------\n\n    logger.info(\"Computing aggregate metrics\")\n    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.countDistinct(\"policy_id\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 5: Custom Calculations\n# MAGIC Derive additional insights.\n\n# COMMAND ----------\n\n    logger.info(\"Deriving additional insights\")\n    final_df = aggregated_df.join(demographics_selected, \"Customer_ID\", \"inner\").withColumn(\n        \"Age\", F.datediff(F.current_date(), F.to_date(\"Date_of_Birth\")) / 365\n    ).withColumn(\n        \"Claim_To_Premium_Ratio\", F.when(F.col(\"total_premium_paid\") != 0, F.col(\"Average_Claim_Amount\") / F.col(\"total_premium_paid\")).otherwise(0)\n    ).withColumn(\n        \"Claims_Per_Policy\", F.when(F.col(\"Policy_Count\") != 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0)\n    ).withColumn(\n        \"Retention_Rate\", F.lit(0.85)\n    ).withColumn(\n        \"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")\n    ).withColumn(\n        \"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\")\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 6: Final Data Assembly\n# MAGIC Combine all transformed data into a single dataset.\n\n# COMMAND ----------\n\n    logger.info(\"Combining all transformed data into a single dataset\")\n    customer_360_df = final_df.join(scores_df, \"Customer_ID\", \"inner\").join(aiml_insights_df, \"Customer_ID\", \"inner\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 7: Output Data\n# MAGIC Write the final dataset to a Unity Catalog table.\n\n# COMMAND ----------\n\n    logger.info(\"Writing the final dataset to a Unity Catalog table\")\n    customer_360_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360\")\n\n    logger.info(\"ETL process completed successfully\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}