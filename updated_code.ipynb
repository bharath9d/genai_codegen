{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process for Global Indicators\n# MAGIC This notebook performs an ETL process on global indicators data using PySpark in Databricks.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, lit, sum as spark_sum\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\ntry:\n    # Step 1: Data Source Configuration\n    logger.info(\"Registering data sources in Databricks Unity Catalog.\")\n    # Assuming tables are already registered in Unity Catalog\n\n    # Step 2: Data Ingestion\n    logger.info(\"Loading data from Unity Catalog tables.\")\n    indicators_df = spark.table(\"catalog.global_world_indicators_2000\")\n    indices_df = spark.table(\"catalog.ConsumerPriceIndices\")\n\n# COMMAND ----------\n\n    # Step 3: Data Transformation\n    logger.info(\"Applying transformations to the data.\")\n\n    # Remove Columns\n    indicators_df = indicators_df.drop(\"UnnecessaryColumn\")\n\n    # Change Column Type\n    indicators_df = indicators_df.withColumn(\"GDP\", col(\"GDP\").cast(\"double\"))\n\n    # Remap\n    indicators_df = indicators_df.withColumn(\"Ease of Business\", when(col(\"Ease of Business\") == \"Easy\", \"1\").otherwise(\"0\"))\n\n    # Add Column\n    indicators_df = indicators_df.withColumn(\"NewColumn\", lit(\"DefaultValue\"))\n\n    # Rename Column\n    indicators_df = indicators_df.withColumnRenamed(\"OldColumnName\", \"NewColumnName\")\n\n    # Filter Operation\n    indicators_df = indicators_df.filter(col(\"Year\") > 2000)\n\n    # Range Filter\n    indicators_df = indicators_df.filter((col(\"GDP\") > 1000) & (col(\"GDP\") < 5000))\n\n    # Aggregate\n    aggregated_df = indicators_df.groupBy(\"Country/Region\").agg(spark_sum(\"GDP\").alias(\"TotalGDP\"))\n\n    # Unpivot\n    unpivoted_df = indicators_df.selectExpr(\n        \"Country/Region\",\n        \"stack(3, 'GDP', GDP, 'CO2 Emissions', CO2_Emissions, 'Health Exp % GDP', Health_Exp_Percent_GDP) as (Indicator, Value)\"\n    )\n\n    # Join Condition\n    joined_df = indicators_df.join(indices_df, indicators_df[\"Country/Region\"] == indices_df[\"Country/Region\"], \"inner\")\n\n# COMMAND ----------\n\n    # Step 4: Custom Calculations\n    logger.info(\"Performing custom calculations.\")\n    indicators_df = indicators_df.withColumn(\"Year\", col(\"Year\").substr(0, 4))\n\n# COMMAND ----------\n\n    # Step 5: Output Data Configuration\n    logger.info(\"Writing transformed data to Unity Catalog as a Delta table.\")\n    final_df = joined_df  # Assuming joined_df is the final DataFrame after all transformations\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.Global_Indicators\")\n\n    logger.info(\"ETL process completed successfully.\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process.\", exc_info=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}