{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process with PySpark\n# MAGIC This notebook performs an ETL process using PySpark, extracting data from a source table, cleaning and transforming it, and then loading it into a target table.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\n# Configuration settings\nsource_table = \"catalog.source_db.source_table\"\ntarget_table = \"catalog.target_db.target_table\"\ncolumns_to_check = [\"column1\", \"column2\"]\ndefault_values = {\"column1\": \"default_value\", \"column2\": 0}\n\n# COMMAND ----------\n\ntry:\n    # Step 1: Data Extraction\n    logger.info(f\"Starting data extraction from Unity Catalog table: {source_table}\")\n    source_df = spark.table(source_table)\n    logger.info(f\"Data extraction completed successfully. Number of records: {source_df.count()}\")\n\n# COMMAND ----------\n\n    # Step 2: Data Cleaning - Remove Invalid Entries\n    logger.info(\"Starting data cleaning by removing invalid entries.\")\n    cleaned_df = source_df.dropna(subset=columns_to_check)\n    logger.info(f\"Invalid entries removed. Number of records after cleaning: {cleaned_df.count()}\")\n\n# COMMAND ----------\n\n    # Step 3: Handle Missing Values\n    logger.info(\"Handling missing values by imputing with default values.\")\n    filled_df = cleaned_df.fillna(default_values)\n    logger.info(\"Missing values handled successfully.\")\n\n# COMMAND ----------\n\n    # Step 4: Convert Data Types\n    logger.info(\"Converting data types for accuracy.\")\n    if \"column1\" in filled_df.columns:\n        converted_df = filled_df.withColumn(\"column1\", filled_df[\"column1\"].cast(IntegerType()))\n    else:\n        logger.warning(\"Column 'column1' not found for type conversion.\")\n        converted_df = filled_df\n    logger.info(\"Data type conversion completed successfully.\")\n\n# COMMAND ----------\n\n    # Step 5: Data Loading\n    logger.info(f\"Loading transformed data into Unity Catalog target table: {target_table}\")\n    converted_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n    logger.info(\"Data loading completed successfully.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred during the ETL process: {e}\")\n    raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Performance Optimizations\n# MAGIC Consider caching the DataFrame if it is reused multiple times, using broadcast joins for small tables, and applying predicate pushdown and column pruning for performance improvements.\n\n# COMMAND ----------\n\n# Performance Optimizations\n# Cache the DataFrame if it is reused multiple times\n# cleaned_df.cache()\n\n# Use broadcast join if applicable\n# small_df = spark.table(\"catalog.db.small_table\")\n# broadcasted_df = broadcast(small_df)\n# joined_df = converted_df.join(broadcasted_df, \"join_key\")\n\n# Apply predicate pushdown and column pruning\n# selected_df = converted_df.select(\"column1\", \"column2\").filter(F.col(\"column1\") > 0)\n\nlogger.info(\"ETL process completed successfully.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}