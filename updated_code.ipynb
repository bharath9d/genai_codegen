{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process with PySpark\n# MAGIC This notebook demonstrates an ETL process using PySpark, including data loading, transformation, and writing to a Unity Catalog target table.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.utils import AnalysisException\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Function Definitions\n# MAGIC Define functions for column validation, data loading, transformation, custom calculations, and data writing.\n\n# COMMAND ----------\n\ndef validate_columns(df, required_columns):\n    \"\"\"Check if required columns exist in the DataFrame.\"\"\"\n    missing_columns = [col for col in required_columns if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing required columns: {missing_columns}\")\n\n# COMMAND ----------\n\ndef load_data():\n    \"\"\"Load data from Unity Catalog source table.\"\"\"\n    logger.info(\"Loading data from Unity Catalog source table.\")\n    return spark.table(\"catalog.source_db.source_table\")\n\n# COMMAND ----------\n\ndef transform_data(df):\n    \"\"\"Apply transformations to the source data.\"\"\"\n    logger.info(\"Applying transformations to the source data.\")\n    validate_columns(df, ['column_name', 'existing_column'])\n    return df.filter(F.col('column_name') > 0) \\\n             .withColumn('new_column', F.col('existing_column') * 2)\n\n# COMMAND ----------\n\ndef apply_custom_calculations(df):\n    \"\"\"Apply custom calculations.\"\"\"\n    logger.info(\"Applying custom calculations.\")\n    validate_columns(df, ['value', 'total'])\n    return df.withColumn('custom_metric', F.col('value') / F.col('total'))\n\n# COMMAND ----------\n\ndef write_data(df):\n    \"\"\"Write transformed data to Unity Catalog target table.\"\"\"\n    logger.info(\"Writing transformed data to Unity Catalog target table.\")\n    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.target_table\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Main ETL Process\n# MAGIC Execute the ETL process by loading data, transforming it, applying custom calculations, and writing the results.\n\n# COMMAND ----------\n\ndef main():\n    try:\n        # Load data\n        source_df = load_data()\n\n        # Transform data\n        transformed_df = transform_data(source_df)\n\n        # Apply custom calculations\n        calculated_df = apply_custom_calculations(transformed_df)\n\n        # Write data\n        write_data(calculated_df)\n\n        logger.info(\"ETL process completed successfully.\")\n\n    except AnalysisException as ae:\n        logger.error(\"Analysis error during the ETL process: %s\", ae, exc_info=True)\n    except ValueError as ve:\n        logger.error(\"Validation error: %s\", ve, exc_info=True)\n    except Exception as e:\n        logger.error(\"An unexpected error occurred during the ETL process.\", exc_info=True)\n\nif __name__ == \"__main__\":\n    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}