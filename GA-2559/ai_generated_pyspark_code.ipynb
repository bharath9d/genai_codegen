{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad5cf28-d210-4bd3-a81a-1a9d9a004d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, broadcast\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Assume the Spark session is already available as 'spark'\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 1: Load Data from Unity Catalog Tables\n",
    "try:\n",
    "    logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "    hospital_stats_df = spark.table(\"genai_demo.cardinal_health.hospital_stats\")\n",
    "    employment_details_df = spark.table(\"genai_demo.cardinal_health.employment_details\")\n",
    "    compensation_guidelines_df = spark.table(\"genai_demo.cardinal_health.db.compensation_guidelines\")\n",
    "    hospital_sales_assignments_df = spark.table(\"genai_demo.cardinal_health.db.hospital_sales_assignments\")\n",
    "    logistics_channels_df = spark.table(\"genai_demo.cardinal_health.db.logistics_channels\")\n",
    "    growth_opportunities_df = spark.table(\"genai_demo.cardinal_health.db.growth_opportunities\")\n",
    "    historical_sales_df = spark.table(\"genai_demo.cardinal_health.db.historical_sales\")\n",
    "    company_goals_df = spark.table(\"genai_demo.cardinal_health.db.company_goals\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data from Unity Catalog: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 2: Data Joining\n",
    "try:\n",
    "    logger.info(\"Performing data joins...\")\n",
    "    # Select necessary columns and join employment details with compensation guidelines\n",
    "    employment_details_df = employment_details_df.select(\"Associate_ID\", \"Associate_Name\", \"Years_of_Experience\")\n",
    "    compensation_guidelines_df = compensation_guidelines_df.select(\"Associate_ID\", \"Base_Salary\", \"Commission_Percentage\", \"Bonus\")\n",
    "    \n",
    "    employment_compensation_df = employment_details_df.join(\n",
    "        broadcast(compensation_guidelines_df), \"Associate_ID\", \"inner\"\n",
    "    ).cache()\n",
    "\n",
    "    # Select necessary columns and join hospital stats with hospital sales assignments\n",
    "    hospital_stats_df = hospital_stats_df.select(\"Hospital_ID\", \"Hospital_Name\", \"Number_of_Beds\", \"Annual_Revenue\", \"Patient_Satisfaction_Score\")\n",
    "    hospital_sales_assignments_df = hospital_sales_assignments_df.select(\"Hospital_ID\", \"Hospital_Name\", \"Associate_ID\", \"Associate_Name\")\n",
    "    \n",
    "    hospital_sales_df = hospital_stats_df.join(\n",
    "        hospital_sales_assignments_df, [\"Hospital_ID\", \"Hospital_Name\"], \"inner\"\n",
    "    ).cache()\n",
    "\n",
    "    # Join the above results on Associate_ID and Associate_Name\n",
    "    combined_df = employment_compensation_df.join(\n",
    "        hospital_sales_df, [\"Associate_ID\", \"Associate_Name\"], \"inner\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data joining: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 3: Custom Calculations\n",
    "try:\n",
    "    logger.info(\"Performing custom calculations...\")\n",
    "    # Calculate total compensation\n",
    "    combined_df = combined_df.withColumn(\n",
    "        \"Total_Compensation\",\n",
    "        col(\"Base_Salary\") + (col(\"Commission_Percentage\") / 100) * col(\"Base_Salary\") + col(\"Bonus\")\n",
    "    )\n",
    "\n",
    "    # Select necessary columns and join logistics channels with growth opportunities\n",
    "    logistics_channels_df = logistics_channels_df.select(\"Channel_ID\", \"Channel_Type\", \"Hospital_ID\")\n",
    "    growth_opportunities_df = growth_opportunities_df.select(\"Channel_ID\", \"Channel_Type\", \"Projected_Growth_Rate\")\n",
    "    \n",
    "    logistics_growth_df = logistics_channels_df.join(\n",
    "        growth_opportunities_df, [\"Channel_ID\", \"Channel_Type\"], \"inner\"\n",
    "    ).cache()\n",
    "\n",
    "    # Join with combined_df on Hospital_ID\n",
    "    final_df = combined_df.join(\n",
    "        logistics_growth_df, \"Hospital_ID\", \"inner\"\n",
    "    )\n",
    "\n",
    "    # Calculate projected revenue\n",
    "    final_df = final_df.withColumn(\n",
    "        \"Projected_Revenue\",\n",
    "        expr(\"\"\"\n",
    "            CASE\n",
    "                WHEN Target_Year = 2024 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n",
    "                WHEN Target_Year = 2025 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n",
    "                WHEN Target_Year = 2026 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n",
    "                ELSE Sales_Revenue\n",
    "            END\n",
    "        \"\"\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during custom calculations: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 4: Filtering and Sorting\n",
    "try:\n",
    "    logger.info(\"Filtering and sorting data...\")\n",
    "    # Filter records where Target Year is greater than 2023\n",
    "    filtered_df = final_df.filter(col(\"Target_Year\") > 2023)\n",
    "\n",
    "    # Sort records by Target Year in ascending order\n",
    "    sorted_df = filtered_df.orderBy(\"Target_Year\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during filtering and sorting: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "# Step 5: Write Output to Unity Catalog\n",
    "try:\n",
    "    logger.info(\"Writing output to Unity Catalog...\")\n",
    "    # Ensure the target database exists\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS catalog.target_db\")\n",
    "    \n",
    "    sorted_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.target_sales\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error writing output to Unity Catalog: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(\"ETL process completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
