{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# COMMAND ----------\nimport logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, expr, broadcast\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already available as 'spark'\n\n# COMMAND ----------\n# Step 1: Load Data from Unity Catalog Tables\ntry:\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    hospital_stats_df = spark.table(\"catalog.db.hospital_stats\")\n    employment_details_df = spark.table(\"catalog.db.employment_details\")\n    compensation_guidelines_df = spark.table(\"catalog.db.compensation_guidelines\")\n    hospital_sales_assignments_df = spark.table(\"catalog.db.hospital_sales_assignments\")\n    logistics_channels_df = spark.table(\"catalog.db.logistics_channels\")\n    growth_opportunities_df = spark.table(\"catalog.db.growth_opportunities\")\n    historical_sales_df = spark.table(\"catalog.db.historical_sales\")\n    company_goals_df = spark.table(\"catalog.db.company_goals\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 2: Data Joining\ntry:\n    logger.info(\"Performing data joins...\")\n    # Select necessary columns and join employment details with compensation guidelines\n    employment_details_df = employment_details_df.select(\"Associate_ID\", \"Associate_Name\", \"Years_of_Experience\")\n    compensation_guidelines_df = compensation_guidelines_df.select(\"Associate_ID\", \"Base_Salary\", \"Commission_Percentage\", \"Bonus\")\n    \n    employment_compensation_df = employment_details_df.join(\n        broadcast(compensation_guidelines_df), \"Associate_ID\", \"inner\"\n    ).cache()\n\n    # Select necessary columns and join hospital stats with hospital sales assignments\n    hospital_stats_df = hospital_stats_df.select(\"Hospital_ID\", \"Hospital_Name\", \"Number_of_Beds\", \"Annual_Revenue\", \"Patient_Satisfaction_Score\")\n    hospital_sales_assignments_df = hospital_sales_assignments_df.select(\"Hospital_ID\", \"Hospital_Name\", \"Associate_ID\", \"Associate_Name\")\n    \n    hospital_sales_df = hospital_stats_df.join(\n        hospital_sales_assignments_df, [\"Hospital_ID\", \"Hospital_Name\"], \"inner\"\n    ).cache()\n\n    # Join the above results on Associate_ID and Associate_Name\n    combined_df = employment_compensation_df.join(\n        hospital_sales_df, [\"Associate_ID\", \"Associate_Name\"], \"inner\"\n    )\nexcept Exception as e:\n    logger.error(f\"Error during data joining: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 3: Custom Calculations\ntry:\n    logger.info(\"Performing custom calculations...\")\n    # Calculate total compensation\n    combined_df = combined_df.withColumn(\n        \"Total_Compensation\",\n        col(\"Base_Salary\") + (col(\"Commission_Percentage\") / 100) * col(\"Base_Salary\") + col(\"Bonus\")\n    )\n\n    # Select necessary columns and join logistics channels with growth opportunities\n    logistics_channels_df = logistics_channels_df.select(\"Channel_ID\", \"Channel_Type\", \"Hospital_ID\")\n    growth_opportunities_df = growth_opportunities_df.select(\"Channel_ID\", \"Channel_Type\", \"Projected_Growth_Rate\")\n    \n    logistics_growth_df = logistics_channels_df.join(\n        growth_opportunities_df, [\"Channel_ID\", \"Channel_Type\"], \"inner\"\n    ).cache()\n\n    # Join with combined_df on Hospital_ID\n    final_df = combined_df.join(\n        logistics_growth_df, \"Hospital_ID\", \"inner\"\n    )\n\n    # Calculate projected revenue\n    final_df = final_df.withColumn(\n        \"Projected_Revenue\",\n        expr(\"\"\"\n            CASE\n                WHEN Target_Year = 2024 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n                WHEN Target_Year = 2025 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n                WHEN Target_Year = 2026 THEN Sales_Revenue * (1 + Projected_Growth_Rate / 100)\n                ELSE Sales_Revenue\n            END\n        \"\"\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error during custom calculations: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 4: Filtering and Sorting\ntry:\n    logger.info(\"Filtering and sorting data...\")\n    # Filter records where Target Year is greater than 2023\n    filtered_df = final_df.filter(col(\"Target_Year\") > 2023)\n\n    # Sort records by Target Year in ascending order\n    sorted_df = filtered_df.orderBy(\"Target_Year\")\nexcept Exception as e:\n    logger.error(f\"Error during filtering and sorting: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 5: Write Output to Unity Catalog\ntry:\n    logger.info(\"Writing output to Unity Catalog...\")\n    # Ensure the target database exists\n    spark.sql(\"CREATE DATABASE IF NOT EXISTS catalog.target_db\")\n    \n    sorted_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.target_sales\")\nexcept Exception as e:\n    logger.error(f\"Error writing output to Unity Catalog: {e}\")\n    raise\n\nlogger.info(\"ETL process completed successfully.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}