{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0c3b6b-8ef7-45cf-98f7-df5da93ab3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Customer 360 Data\n",
    "# MAGIC This notebook performs an ETL process to create a Customer 360 view by integrating data from various Unity Catalog tables.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import logging\n",
    "from pyspark.sql.functions import col, count, max, avg, when, lit, datediff, current_date\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    # Load data from Unity Catalog tables\n",
    "    logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "    policy_df = spark.table(\"genai_demo.jnj.policy\")\n",
    "    claims_df = spark.table(\"genai_demo.jnj.claims\")\n",
    "    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n",
    "    scores_df = spark.table(\"genai_demo.jnj.scores\")\n",
    "    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Select relevant fields from demographics data\n",
    "    logger.info(\"Selecting relevant fields from demographics data...\")\n",
    "    selected_demographics_df = demographics_df.select(\n",
    "        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \n",
    "        \"City\", \"State\", \"Postal_Code\", \"Date_of_Birth\", \"Gender\", \n",
    "        \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Join demographics with policy data\n",
    "    logger.info(\"Joining demographics with policy data...\")\n",
    "    joined_df = selected_demographics_df.join(\n",
    "        policy_df, selected_demographics_df.Customer_ID == policy_df.customer_id, \"inner\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Remove duplicate columns from claims_df before joining\n",
    "    claims_df = claims_df.drop(\"Policy_ID\")\n",
    "\n",
    "    # Join the result with claims data\n",
    "    logger.info(\"Joining the result with claims data...\")\n",
    "    joined_df = joined_df.join(\n",
    "        claims_df, joined_df.policy_id == claims_df.Policy_ID, \"inner\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Aggregate data\n",
    "    logger.info(\"Aggregating data...\")\n",
    "    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n",
    "        count(\"Claim_ID\").alias(\"Total_Claims\"),\n",
    "        count(\"policy_id\").alias(\"Policy_Count\"),\n",
    "        max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n",
    "        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Custom calculations\n",
    "    logger.info(\"Performing custom calculations...\")\n",
    "    enriched_df = aggregated_df.withColumn(\n",
    "        \"Age\", datediff(current_date(), col(\"Date_of_Birth\")) / 365\n",
    "    ).withColumn(\n",
    "        \"Claim_To_Premium_Ratio\", when(col(\"total_premium_paid\") != 0, col(\"Average_Claim_Amount\") / col(\"total_premium_paid\")).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"Claims_Per_Policy\", when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"Retention_Rate\", lit(0.85)\n",
    "    ).withColumn(\n",
    "        \"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\")\n",
    "    ).withColumn(\n",
    "        \"Upsell_Potential\", lit(\"Premium Vehicle Coverage\")\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Integrate AI/ML insights and customer scores\n",
    "    logger.info(\"Integrating AI/ML insights and customer scores...\")\n",
    "    final_df = enriched_df.join(\n",
    "        aiml_insights_df, \"Customer_ID\", \"inner\"\n",
    "    ).join(\n",
    "        scores_df, \"Customer_ID\", \"inner\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Write the final DataFrame to a Unity Catalog table\n",
    "    logger.info(\"Writing the final DataFrame to a Unity Catalog table...\")\n",
    "    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.jnj.customer_360\")\n",
    "\n",
    "    logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
