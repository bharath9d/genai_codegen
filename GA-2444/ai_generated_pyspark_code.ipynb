{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # Customer 360 View ETL Process\n# MAGIC This notebook performs an ETL process to create a comprehensive Customer 360 View using data from Unity Catalog tables.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType, DateType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 1: Data Selection and Filtering\n# MAGIC Load and select data from Unity Catalog tables.\n\n# COMMAND ----------\n\ntry:\n    logger.info(\"Loading and selecting data from Unity Catalog tables.\")\n    \n    demographics_df = spark.table(\"genai_demo.jnj.demographics\").select(\n        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\",\n        \"State\", \"Postal_Code\", \"Date_of_Birth\", \"Gender\", \"Marital_Status\",\n        \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n    ).withColumn(\"Date_of_Birth\", F.col(\"Date_of_Birth\").cast(DateType()))\n\n    claims_df = spark.table(\"genai_demo.jnj.claims\").select(\n        \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\",\n        \"Claim_Amount\", \"Claim_Payout\"\n    ).withColumn(\"Claim_Date\", F.col(\"Claim_Date\").cast(DateType())) \\\n     .withColumn(\"Claim_Amount\", F.col(\"Claim_Amount\").cast(DoubleType())) \\\n     .withColumn(\"Claim_Payout\", F.col(\"Claim_Payout\").cast(DoubleType()))\n\n    policy_df = spark.table(\"genai_demo.jnj.policy\").select(\n        \"Policy_ID\", \"Customer_ID\", \"Policy_Type\", \"Policy_Status\", \"Policy_Start_Date\",\n        \"Policy_End_Date\", \"Policy_Term\", \"Policy_Premium\", \"Total_Premium_Paid\",\n        \"Renewal_Status\", \"Policy_Addons\"\n    ).withColumn(\"Policy_Start_Date\", F.col(\"Policy_Start_Date\").cast(DateType())) \\\n     .withColumn(\"Policy_End_Date\", F.col(\"Policy_End_Date\").cast(DateType())) \\\n     .withColumn(\"Policy_Premium\", F.col(\"Policy_Premium\").cast(DoubleType())) \\\n     .withColumn(\"Total_Premium_Paid\", F.col(\"Total_Premium_Paid\").cast(DoubleType()))\n\n    scores_df = spark.table(\"genai_demo.jnj.scores\").select(\n        \"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\"\n    )\n\n    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\").select(\n        \"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \"Claims_Fraud_Probability\", \"Revenue_Potential\"\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 2: Data Integration\n# MAGIC Integrate datasets based on common identifiers.\n\n# COMMAND ----------\n\n    logger.info(\"Integrating datasets based on common identifiers.\")\n    \n    integrated_df = demographics_df.join(policy_df, \"Customer_ID\", \"inner\")\n    integrated_df = integrated_df.join(claims_df, \"Policy_ID\", \"inner\")\n\n    # Cache the integrated DataFrame as it will be used multiple times\n    integrated_df.cache()\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 3: Data Aggregation\n# MAGIC Aggregate data to compute metrics.\n\n# COMMAND ----------\n\n    logger.info(\"Aggregating data to compute metrics.\")\n    \n    aggregated_df = integrated_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.count(\"Policy_ID\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 4: Custom Calculations\n# MAGIC Perform custom calculations on the aggregated data.\n\n# COMMAND ----------\n\n    logger.info(\"Performing custom calculations.\")\n    \n    aggregated_df = aggregated_df.withColumn(\n        \"Age\", F.datediff(F.current_date(), \"Date_of_Birth\") / 365\n    ).withColumn(\n        \"Claim_To_Premium_Ratio\",\n        F.when(aggregated_df[\"Total_Premium_Paid\"] != 0,\n               aggregated_df[\"Average_Claim_Amount\"] / aggregated_df[\"Total_Premium_Paid\"]).otherwise(0)\n    ).withColumn(\n        \"Claims_Per_Policy\",\n        F.when(aggregated_df[\"Policy_Count\"] != 0,\n               aggregated_df[\"Total_Claims\"] / aggregated_df[\"Policy_Count\"]).otherwise(0)\n    ).withColumn(\n        \"Retention_Rate\", F.lit(0.85)\n    ).withColumn(\n        \"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")\n    ).withColumn(\n        \"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\")\n    )\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 5: Comprehensive Data Joining\n# MAGIC Join with AI/ML insights and scores.\n\n# COMMAND ----------\n\n    logger.info(\"Joining with AI/ML insights and scores.\")\n    \n    final_df = aggregated_df.join(aiml_insights_df, \"Customer_ID\", \"inner\")\n    final_df = final_df.join(scores_df, \"Customer_ID\", \"inner\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Step 6: Output Generation\n# MAGIC Write the final Customer 360 View to Unity Catalog.\n\n# COMMAND ----------\n\n    logger.info(\"Writing the final Customer 360 View to Unity Catalog.\")\n    \n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.guardian.customer_360_view\")\n\n    logger.info(\"ETL process completed successfully.\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process: %s\", e)\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}