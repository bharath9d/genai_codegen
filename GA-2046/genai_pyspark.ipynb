{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process for Superstore Sales Data\n# MAGIC This notebook performs an ETL process on Superstore sales data using PySpark. It includes data loading, cleansing, integration, restructuring, aggregation, and writing the results back to Unity Catalog tables.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType, StringType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\ndef load_data():\n    \"\"\"Load data from Unity Catalog tables.\"\"\"\n    logger.info(\"Loading data from Unity Catalog tables\")\n    orders_central_df = spark.table(\"catalog.db.orders_central\")\n    orders_west_df = spark.table(\"catalog.db.orders_west\")\n    orders_east_df = spark.table(\"catalog.db.orders_east\")\n    orders_south_df = spark.table(\"catalog.db.orders_south\")\n    quota_df = spark.table(\"catalog.db.quota\")\n    returns_df = spark.table(\"catalog.db.returns\")\n    return orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df\n\n# COMMAND ----------\n\ndef cleanse_and_standardize(orders_df):\n    \"\"\"Perform data cleansing and standardization.\"\"\"\n    logger.info(\"Performing data cleansing and standardization\")\n    orders_df = orders_df.withColumn(\n        \"Order Date\",\n        F.to_date(F.concat_ws(\"/\", F.col(\"Order Day\"), F.col(\"Order Month\"), F.col(\"Order Year\")), \"dd/MM/yyyy\")\n    ).withColumnRenamed(\"Discounts\", \"Discount\")\n    orders_df = orders_df.drop(\"Order Day\", \"Order Month\", \"Order Year\", \"Ship Day\", \"Ship Month\", \"Ship Year\")\n    orders_df = orders_df.withColumn(\"Discount\", F.col(\"Discount\").cast(StringType()))\n    orders_df = orders_df.withColumn(\"Sales\", F.regexp_replace(F.col(\"Sales\"), r'[:Letter:]', '').cast(DoubleType()))\n    return orders_df\n\n# COMMAND ----------\n\ndef integrate_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df):\n    \"\"\"Integrate data from multiple sources.\"\"\"\n    logger.info(\"Integrating data from multiple sources\")\n    all_orders_df = orders_central_df.union(orders_west_df).union(orders_east_df).union(orders_south_df)\n    all_orders_df.cache()\n    orders_returns_df = all_orders_df.join(F.broadcast(returns_df), [\"Order ID\", \"Product ID\"], \"right\")\n    return orders_returns_df\n\n# COMMAND ----------\n\ndef restructure_data(quota_df, orders_returns_df):\n    \"\"\"Restructure data.\"\"\"\n    logger.info(\"Restructuring data\")\n    quota_unpivoted_df = quota_df.selectExpr(\"Region\", \"stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`) as (Year, Quota)\")\n    state_mapping = {\"Arizona\": \"AZ\", \"California\": \"CA\"}\n    orders_returns_df = orders_returns_df.replace(state_mapping, subset=[\"State\"])\n    return quota_unpivoted_df, orders_returns_df\n\n# COMMAND ----------\n\ndef perform_aggregations(orders_returns_df):\n    \"\"\"Perform aggregations and custom calculations.\"\"\"\n    logger.info(\"Performing aggregations and custom calculations\")\n    aggregated_sales_df = orders_returns_df.groupBy(\"Region\", F.year(\"Order Date\").alias(\"Year of Sale\")).agg(\n        F.sum(\"Profit\").alias(\"Total Profit\"),\n        F.sum(\"Sales\").alias(\"Total Sales\"),\n        F.sum(\"Quantity\").alias(\"Total Quantity\"),\n        F.avg(\"Discount\").alias(\"Average Discount\")\n    )\n    orders_returns_df = orders_returns_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n    orders_returns_df = orders_returns_df.withColumn(\"Returned?\", F.expr(\"IF(ISNULL(Return Reason), 'No', 'Yes')\"))\n    return aggregated_sales_df, orders_returns_df\n\n# COMMAND ----------\n\ndef write_data(aggregated_sales_df, orders_returns_df):\n    \"\"\"Write transformed data to Unity Catalog tables.\"\"\"\n    logger.info(\"Writing transformed data to Unity Catalog tables\")\n    aggregated_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.annual_regional_performance\")\n    orders_returns_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.superstore_sales\")\n\n# COMMAND ----------\n\ntry:\n    # Load data\n    orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df = load_data()\n\n    # Cleanse and standardize data\n    orders_central_df = cleanse_and_standardize(orders_central_df)\n    orders_west_df = cleanse_and_standardize(orders_west_df)\n    orders_east_df = cleanse_and_standardize(orders_east_df)\n    orders_south_df = cleanse_and_standardize(orders_south_df)\n\n    # Integrate data\n    orders_returns_df = integrate_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df)\n\n    # Restructure data\n    quota_unpivoted_df, orders_returns_df = restructure_data(quota_df, orders_returns_df)\n\n    # Perform aggregations\n    aggregated_sales_df, orders_returns_df = perform_aggregations(orders_returns_df)\n\n    # Write data\n    write_data(aggregated_sales_df, orders_returns_df)\n\n    logger.info(\"ETL process completed successfully\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}