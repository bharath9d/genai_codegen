{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# COMMAND ----------\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType, StringType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n# Step 1: Data Source Configuration\n# Assume data sources are registered in Unity Catalog and accessible via Spark SQL\n\n# COMMAND ----------\n# Step 2: Data Ingestion\ntry:\n    # Reading data from Unity Catalog tables\n    orders_central_df = spark.table(\"catalog.db.orders_central\")\n    orders_west_df = spark.table(\"catalog.db.orders_west\")\n    orders_east_df = spark.table(\"catalog.db.orders_east\")\n    orders_south_df = spark.table(\"catalog.db.orders_south\")\n    quota_df = spark.table(\"catalog.db.quota\")\n    returns_df = spark.table(\"catalog.db.returns\")\n    logger.info(\"Data ingestion completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data ingestion: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 3: Data Cleansing and Standardization\ntry:\n    # Adjusting date fields and standardizing column names\n    orders_central_df = orders_central_df.withColumn(\n        \"Order Date\",\n        F.to_date(F.concat_ws(\"/\", F.col(\"Order Day\"), F.col(\"Order Month\"), F.col(\"Order Year\")), \"dd/MM/yyyy\")\n    ).drop(\"Order Day\", \"Order Month\", \"Order Year\")\n\n    orders_central_df = orders_central_df.withColumnRenamed(\"Discounts\", \"Discount\")\n    logger.info(\"Data cleansing and standardization completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data cleansing and standardization: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 4: Data Type Adjustments\ntry:\n    # Converting data types\n    orders_central_df = orders_central_df.withColumn(\"Discount\", F.col(\"Discount\").cast(StringType()))\n    orders_central_df = orders_central_df.withColumn(\"Sales\", F.col(\"Sales\").cast(DoubleType()))\n    logger.info(\"Data type adjustments completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data type adjustments: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 5: Data Integration\ntry:\n    # Union datasets\n    all_orders_df = orders_central_df.unionByName(orders_west_df).unionByName(orders_east_df).unionByName(orders_south_df)\n\n    # Join with returns\n    orders_returns_df = all_orders_df.join(returns_df, [\"Order ID\", \"Product ID\"], \"right\")\n    logger.info(\"Data integration completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data integration: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 6: Data Restructuring\ntry:\n    # Unpivot quota data\n    quota_unpivoted_df = quota_df.selectExpr(\"Region\", \"stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`) as (Year, Quota)\")\n\n    # Standardize state names\n    state_mapping = {\"Arizona\": \"AZ\", \"California\": \"CA\"}\n    orders_returns_df = orders_returns_df.replace(state_mapping, subset=[\"State\"])\n    logger.info(\"Data restructuring completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data restructuring: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 7: Aggregations and Calculations\ntry:\n    # Aggregating sales data\n    aggregated_sales_df = orders_returns_df.groupBy(\"Region\", F.year(\"Order Date\").alias(\"Year of Sale\")).agg(\n        F.sum(\"Profit\").alias(\"Total Profit\"),\n        F.sum(\"Sales\").alias(\"Total Sales\"),\n        F.sum(\"Quantity\").alias(\"Total Quantity\"),\n        F.avg(\"Discount\").alias(\"Average Discount\")\n    )\n\n    # Custom calculations\n    orders_returns_df = orders_returns_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n    orders_returns_df = orders_returns_df.withColumn(\"Returned?\", F.expr(\"IF(ISNULL(Return Reason), 'No', 'Yes')\"))\n    logger.info(\"Aggregations and calculations completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during aggregations and calculations: {e}\")\n    raise\n\n# COMMAND ----------\n# Step 8: Output Data Generation\ntry:\n    # Writing to Databricks tables\n    aggregated_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.annual_regional_performance\")\n    orders_returns_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.db.superstore_sales\")\n    logger.info(\"Output data generation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during output data generation: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}