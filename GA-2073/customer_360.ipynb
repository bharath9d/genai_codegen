{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # Data Processing with PySpark\n# MAGIC This notebook demonstrates loading, transforming, and writing data using PySpark in a Databricks environment.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql.functions import col, to_date, count, avg, max, datediff, current_date, when, lit, broadcast\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already initialized as 'spark'\n\n# COMMAND ----------\n\ndef load_data():\n    try:\n        # Load data from CSV files into DataFrames\n        policy_df = spark.read.csv(\"dbfs:/path/to/policy.csv\", header=True, inferSchema=True)\n        claims_df = spark.read.csv(\"dbfs:/path/to/claims.csv\", header=True, inferSchema=True)\n        demographics_df = spark.read.csv(\"dbfs:/path/to/demographics.csv\", header=True, inferSchema=True)\n        scores_df = spark.read.csv(\"dbfs:/path/to/scores.csv\", header=True, inferSchema=True)\n        aiml_insights_df = spark.read.csv(\"dbfs:/path/to/aiml_insights.csv\", header=True, inferSchema=True)\n\n        # Register DataFrames as temporary views\n        policy_df.createOrReplaceTempView(\"policy\")\n        claims_df.createOrReplaceTempView(\"claims\")\n        demographics_df.createOrReplaceTempView(\"demographics\")\n        scores_df.createOrReplaceTempView(\"scores\")\n        aiml_insights_df.createOrReplaceTempView(\"aiml_insights\")\n\n        logger.info(\"Data loaded and temporary views created successfully.\")\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\n# COMMAND ----------\n\ndef transform_data():\n    try:\n        # Convert data types\n        demographics_df = spark.sql(\"\"\"\n            SELECT *, to_date(Date_of_Birth, 'yyyy-MM-dd') as Date_of_Birth\n            FROM demographics\n        \"\"\")\n        claims_df = spark.sql(\"\"\"\n            SELECT *, to_date(Claim_Date, 'yyyy-MM-dd') as Claim_Date,\n                   cast(Claim_Amount as double) as Claim_Amount,\n                   cast(Claim_Payout as double) as Claim_Payout\n            FROM claims\n        \"\"\")\n        policy_df = spark.sql(\"\"\"\n            SELECT *, to_date(policy_start_date, 'yyyy-MM-dd') as policy_start_date,\n                   to_date(policy_end_date, 'yyyy-MM-dd') as policy_end_date,\n                   cast(policy_premium as double) as policy_premium,\n                   cast(total_premium_paid as double) as total_premium_paid\n            FROM policy\n        \"\"\")\n\n        # Join demographics and policy data\n        customer_policy_df = demographics_df.join(policy_df, demographics_df.Customer_ID == policy_df.customer_id, \"inner\")\n\n        # Drop duplicate columns from the right DataFrame\n        customer_policy_df = customer_policy_df.drop(policy_df.customer_id)\n\n        # Join with claims data\n        customer_policy_claims_df = customer_policy_df.join(claims_df, customer_policy_df.policy_id == claims_df.Policy_ID, \"inner\")\n\n        # Drop duplicate columns from the right DataFrame\n        customer_policy_claims_df = customer_policy_claims_df.drop(claims_df.Policy_ID)\n\n        # Aggregate data\n        aggregated_df = customer_policy_claims_df.groupBy(\"Customer_ID\").agg(\n            count(\"Claim_ID\").alias(\"Total_Claims\"),\n            count(\"policy_id\").alias(\"Policy_Count\"),\n            max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n            avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n        )\n\n        # Custom Calculations\n        aggregated_df = aggregated_df.withColumn(\"Age\", datediff(current_date(), col(\"Date_of_Birth\")) / 365)\n        aggregated_df = aggregated_df.withColumn(\"Claim_To_Premium_Ratio\", \n                                                 when(col(\"total_premium_paid\") != 0, col(\"Claim_Amount\") / col(\"total_premium_paid\")).otherwise(0))\n        aggregated_df = aggregated_df.withColumn(\"Claims_Per_Policy\", \n                                                 when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0))\n        aggregated_df = aggregated_df.withColumn(\"Retention_Rate\", lit(0.85))\n        aggregated_df = aggregated_df.withColumn(\"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\"))\n        aggregated_df = aggregated_df.withColumn(\"Upsell_Potential\", lit(\"Premium Vehicle Coverage\"))\n\n        logger.info(\"Data transformation completed successfully.\")\n        return aggregated_df\n    except Exception as e:\n        logger.error(f\"Error during data transformation: {e}\")\n        raise\n\n# COMMAND ----------\n\ndef write_data(aggregated_df):\n    try:\n        # Write the final DataFrame to a Delta table\n        aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360\")\n        logger.info(\"Data written to Delta table successfully.\")\n    except Exception as e:\n        logger.error(f\"Error writing data to Delta table: {e}\")\n        raise\n\n# COMMAND ----------\n\ndef main():\n    load_data()\n    transformed_data = transform_data()\n    write_data(transformed_data)\n\nif __name__ == \"__main__\":\n    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}