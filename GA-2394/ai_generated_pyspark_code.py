{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"# Databricks notebook source # MAGIC %md # MAGIC # ETL Process for Superstore Sales Data # MAGIC This notebook performs an ETL process on Superstore sales data using PySpark. It loads data from Unity Catalog, transforms it, and writes the results back to Unity Catalog.  # COMMAND ----------  import logging from pyspark.sql.functions import col, concat, lit, when, datediff, year, regexp_replace, sum, avg, to_date, broadcast  # Initialize logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__)  # COMMAND ----------  def load_data():     try:         # Load data from Unity Catalog tables         orders_central_df = spark.table(\"catalog.source_db.orders_central\")         orders_west_df = spark.table(\"catalog.source_db.orders_west\")         orders_east_df = spark.table(\"catalog.source_db.orders_east\")         orders_south_df = spark.table(\"catalog.source_db.orders_south_2015\")         returns_df = spark.table(\"catalog.source_db.returns_all\")         quota_df = spark.table(\"catalog.source_db.quota\")                  logger.info(\"Data loaded successfully from Unity Catalog tables.\")         return orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df     except Exception as e:         logger.error(f\"Error loading data: {e}\")         raise  # COMMAND ----------  def transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df):     try:         # Data Standardization         orders_central_df = orders_central_df.withColumn(\"Order Date\", to_date(concat(col(\"Order Day\"), lit(\"/\"), col(\"Order Month\"), lit(\"/\"), col(\"Order Year\")), \"d/M/yyyy\")) \\                                              .withColumn(\"Ship Date\", to_date(concat(col(\"Ship Day\"), lit(\"/\"), col(\"Ship Month\"), lit(\"/\"), col(\"Ship Year\")), \"d/M/yyyy\")) \\                                              .drop(\"Order Year\", \"Order Month\", \"Order Day\", \"Ship Year\", \"Ship Month\", \"Ship Day\") \\                                              .withColumnRenamed(\"Discounts\", \"Discount\") \\                                              .withColumnRenamed(\"Product\", \"Product Name\")                  # Data Cleaning         orders_central_df = orders_central_df.filter(orders_central_df[\"Order ID\"].isNotNull()) \\                                              .withColumn(\"Sales\", regexp_replace(col(\"Sales\"), '[:Letter:]', '').cast(\"double\"))                  # Data Consolidation         all_orders_df = orders_central_df.union(orders_west_df).union(orders_east_df).union(orders_south_df)                  # Cache the consolidated DataFrame for performance         all_orders_df.cache()                  # Join with returns data         enriched_orders_df = all_orders_df.join(broadcast(returns_df), \"Order ID\", \"left\")                  # Calculated Fields         enriched_orders_df = enriched_orders_df.withColumn(\"Days to Ship\", datediff(col(\"Ship Date\"), col(\"Order Date\"))) \\                                                .withColumn(\"Returned?\", when(col(\"Return Reason\").isNotNull(), \"Yes\").otherwise(\"No\")) \\                                                .withColumn(\"Year of Sale\", year(col(\"Order Date\")))                  # Business Rules         enriched_orders_df = enriched_orders_df.filter(~(col(\"Discount\").between(17, 18)))                  # Pivoting and Aggregation         quota_df = quota_df.selectExpr(\"Region\", \"stack(4, '2014', 2014, '2015', 2015, '2016', 2016, '2017', 2017) as (Year, Quota)\")                  aggregated_df = enriched_orders_df.groupBy(\"Region\", \"Year of Sale\").agg(             sum(\"Profit\").alias(\"Total Profit\"),             sum(\"Sales\").alias(\"Total Sales\"),             sum(\"Quantity\").alias(\"Total Quantity\"),             avg(\"Discount\").alias(\"Average Discount\")         )                  logger.info(\"Data transformation completed successfully.\")         return enriched_orders_df, aggregated_df     except Exception as e:         logger.error(f\"Error during data transformation: {e}\")         raise  # COMMAND ----------  def write_data(enriched_orders_df, aggregated_df):     try:         # Write to Unity Catalog target tables         enriched_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.superstore_sales\")         aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.annual_regional_performance\")                  logger.info(\"Data written successfully to Unity Catalog target tables.\")     except Exception as e:         logger.error(f\"Error writing data: {e}\")         raise  # COMMAND ----------  def main():     try:         orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df = load_data()         enriched_orders_df, aggregated_df = transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df)         write_data(enriched_orders_df, aggregated_df)     except Exception as e:         logger.error(f\"ETL process failed: {e}\")  # COMMAND ----------  if __name__ == \"__main__\":     main() # Databricks notebook source # MAGIC %md # MAGIC # ETL Process for Superstore Sales Data # MAGIC This notebook performs an ETL process on Superstore sales data using PySpark. It loads data from Unity Catalog, transforms it, and writes the results back to Unity Catalog.  # COMMAND ----------  import logging from pyspark.sql.functions import col, concat, lit, when, datediff, year, regexp_replace, sum, avg, to_date, broadcast  # Initialize logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__)  # COMMAND ----------  def load_data():     try:         # Load data from Unity Catalog tables         orders_central_df = spark.table(\"catalog.source_db.orders_central\")         orders_west_df = spark.table(\"catalog.source_db.orders_west\")         orders_east_df = spark.table(\"catalog.source_db.orders_east\")         orders_south_df = spark.table(\"catalog.source_db.orders_south_2015\")         returns_df = spark.table(\"catalog.source_db.returns_all\")         quota_df = spark.table(\"catalog.source_db.quota\")                  logger.info(\"Data loaded successfully from Unity Catalog tables.\")         return orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df     except Exception as e:         logger.error(f\"Error loading data: {e}\")         raise  # COMMAND ----------  def transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df):     try:         # Data Standardization         orders_central_df = orders_central_df.withColumn(\"Order Date\", to_date(concat(col(\"Order Day\"), lit(\"/\"), col(\"Order Month\"), lit(\"/\"), col(\"Order Year\")), \"d/M/yyyy\")) \\                                              .withColumn(\"Ship Date\", to_date(concat(col(\"Ship Day\"), lit(\"/\"), col(\"Ship Month\"), lit(\"/\"), col(\"Ship Year\")), \"d/M/yyyy\")) \\                                              .drop(\"Order Year\", \"Order Month\", \"Order Day\", \"Ship Year\", \"Ship Month\", \"Ship Day\") \\                                              .withColumnRenamed(\"Discounts\", \"Discount\") \\                                              .withColumnRenamed(\"Product\", \"Product Name\")                  # Data Cleaning         orders_central_df = orders_central_df.filter(orders_central_df[\"Order ID\"].isNotNull()) \\                                              .withColumn(\"Sales\", regexp_replace(col(\"Sales\"), '[:Letter:]', '').cast(\"double\"))                  # Data Consolidation         all_orders_df = orders_central_df.union(orders_west_df).union(orders_east_df).union(orders_south_df)                  # Cache the consolidated DataFrame for performance         all_orders_df.cache()                  # Join with returns data         enriched_orders_df = all_orders_df.join(broadcast(returns_df), \"Order ID\", \"left\")                  # Calculated Fields         enriched_orders_df = enriched_orders_df.withColumn(\"Days to Ship\", datediff(col(\"Ship Date\"), col(\"Order Date\"))) \\                                                .withColumn(\"Returned?\", when(col(\"Return Reason\").isNotNull(), \"Yes\").otherwise(\"No\")) \\                                                .withColumn(\"Year of Sale\", year(col(\"Order Date\")))                  # Business Rules         enriched_orders_df = enriched_orders_df.filter(~(col(\"Discount\").between(17, 18)))                  # Pivoting and Aggregation         quota_df = quota_df.selectExpr(\"Region\", \"stack(4, '2014', 2014, '2015', 2015, '2016', 2016, '2017', 2017) as (Year, Quota)\")                  aggregated_df = enriched_orders_df.groupBy(\"Region\", \"Year of Sale\").agg(             sum(\"Profit\").alias(\"Total Profit\"),             sum(\"Sales\").alias(\"Total Sales\"),             sum(\"Quantity\").alias(\"Total Quantity\"),             avg(\"Discount\").alias(\"Average Discount\")         )                  logger.info(\"Data transformation completed successfully.\")         return enriched_orders_df, aggregated_df     except Exception as e:         logger.error(f\"Error during data transformation: {e}\")         raise  # COMMAND ----------  def write_data(enriched_orders_df, aggregated_df):     try:         # Write to Unity Catalog target tables         enriched_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.superstore_sales\")         aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.annual_regional_performance\")                  logger.info(\"Data written successfully to Unity Catalog target tables.\")     except Exception as e:         logger.error(f\"Error writing data: {e}\")         raise  # COMMAND ----------  def main():     try:         orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df = load_data()         enriched_orders_df, aggregated_df = transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df, quota_df)         write_data(enriched_orders_df, aggregated_df)     except Exception as e:         logger.error(f\"ETL process failed: {e}\")  # COMMAND ----------  if __name__ == \"__main__\":     main()\"\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}