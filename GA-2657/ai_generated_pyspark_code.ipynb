{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478bcb2c-d47e-45c5-8dbc-f4dfd3ab143e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Superstore Sales Data\n",
    "# MAGIC This notebook performs an ETL process on Superstore sales data using PySpark.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, DateType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    # Step 1: Data Loading\n",
    "    logger.info(\"Loading data from Unity Catalog tables.\")\n",
    "    orders_central_df = spark.table(\"genai_demo.citi.orders_central\")\n",
    "    orders_east_df = spark.table(\"genai_demo.citi.orders_east\")\n",
    "    orders_south_2015_df = spark.table(\"genai_demo.citi.orders_south_2015\")\n",
    "    orders_south_2016_df = spark.table(\"genai_demo.citi.orders_south_2016\")\n",
    "    orders_south_2017_df = spark.table(\"genai_demo.citi.orders_south_2017\")\n",
    "    orders_south_2018_df = spark.table(\"genai_demo.citi.orders_south_2018\")\n",
    "    orders_west_df = spark.table(\"genai_demo.citi.orders_west\")\n",
    "    \n",
    "    quota_df = spark.table(\"genai_demo.citi.quota\")\n",
    "    returns_df = spark.table(\"genai_demo.citi.returns\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Helper function to standardize each orders DataFrame.\n",
    "    # Adjust as needed based on your actual column names.\n",
    "    def standardize_orders_df(df):\n",
    "        # 1) Ensure there's a single 'Discount' column\n",
    "        #    If the original column is 'Discounts', rename it to 'Discount'.\n",
    "        #    If columns differ across regions (e.g., 'Discount' in some, 'Discounts' in others),\n",
    "        #    you might need condition checks or “coalesce” logic, etc.\n",
    "        if \"Discounts\" in df.columns:\n",
    "            df = df.withColumnRenamed(\"Discounts\", \"Discount\")\n",
    "        \n",
    "        # 2) Create an actual Date column if the table has separate day/month/year columns\n",
    "        #    OR if it already has “Order Date” in correct date format, just cast it.\n",
    "        #    Below code assumes day/month/year columns exist (like \"Order Day\", \"Order Month\", etc.).\n",
    "        #    If your tables differ, adjust accordingly.\n",
    "        if all(x in df.columns for x in [\"Order Day\", \"Order Month\", \"Order Year\"]):\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    \"Order Date\",\n",
    "                    F.to_date(\n",
    "                        F.concat(\n",
    "                            F.col(\"Order Month\"), F.lit(\"/\"), \n",
    "                            F.col(\"Order Day\"), F.lit(\"/\"), \n",
    "                            F.col(\"Order Year\")\n",
    "                        ),\n",
    "                        \"M/d/yyyy\"  # or \"d/M/yyyy\" if your data is day-first\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # 3) Cast numeric columns\n",
    "        #    e.g. \"Sales\", \"Profit\", \"Quantity\", \"Discount\" if they aren't numeric\n",
    "        for col_name in [\"Sales\", \"Profit\", \"Quantity\", \"Discount\"]:\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(col_name, F.col(col_name).cast(DoubleType()))\n",
    "        \n",
    "        # 4) Similarly, if there's a \"Ship Date\" string that needs to be date, cast it:\n",
    "        if \"Ship Date\" in df.columns:\n",
    "            df = df.withColumn(\"Ship Date\", F.to_date(F.col(\"Ship Date\"), \"M/d/yyyy\"))\n",
    "        \n",
    "        # 5) Return standardized df\n",
    "        return df\n",
    "\n",
    "    # Standardize each orders DataFrame\n",
    "    logger.info(\"Standardizing data.\")\n",
    "    orders_central_df = standardize_orders_df(orders_central_df)\n",
    "    orders_east_df = standardize_orders_df(orders_east_df)\n",
    "    orders_south_2015_df = standardize_orders_df(orders_south_2015_df)\n",
    "    orders_south_2016_df = standardize_orders_df(orders_south_2016_df)\n",
    "    orders_south_2017_df = standardize_orders_df(orders_south_2017_df)\n",
    "    orders_south_2018_df = standardize_orders_df(orders_south_2018_df)\n",
    "    orders_west_df = standardize_orders_df(orders_west_df)\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 2: Data Cleaning\n",
    "    # e.g., filter out null Order IDs\n",
    "    logger.info(\"Cleaning data.\")\n",
    "    orders_central_df = orders_central_df.filter(F.col(\"Order ID\").isNotNull())\n",
    "    # Repeat if needed for other dataframes, or do it post-union. Here we do it post-union to keep consistency.\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 3: Consolidate orders into one DF\n",
    "    logger.info(\"Consolidating all orders via union.\")\n",
    "    all_orders_df = (\n",
    "        orders_central_df\n",
    "        .unionByName(orders_east_df, allowMissingColumns=True)\n",
    "        .unionByName(orders_south_2015_df, allowMissingColumns=True)\n",
    "        .unionByName(orders_south_2016_df, allowMissingColumns=True)\n",
    "        .unionByName(orders_south_2017_df, allowMissingColumns=True)\n",
    "        .unionByName(orders_south_2018_df, allowMissingColumns=True)\n",
    "        .unionByName(orders_west_df, allowMissingColumns=True)\n",
    "    )\n",
    "\n",
    "    # Filter out null order IDs after union (if you want to do it once).\n",
    "    all_orders_df = all_orders_df.filter(F.col(\"Order ID\").isNotNull())\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 4: Add Returns info\n",
    "    # Join returns to have \"Return Reason\" for each order (if it exists).\n",
    "    # Adjust the join column if it's named differently in returns_df.\n",
    "    # Then we can compute \"Returned?\" based on the presence of a Return Reason.\n",
    "    logger.info(\"Joining returns data.\")\n",
    "    if \"Order ID\" in returns_df.columns:\n",
    "        all_orders_df = all_orders_df.join(\n",
    "            returns_df.select(\"Order ID\", \"Return Reason\"),\n",
    "            on=\"Order ID\",\n",
    "            how=\"left\"  # keep all orders, match if there's a return\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"returns_df has no 'Order ID' column. Skipping join with returns.\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 5: Pivoting quota_df if needed\n",
    "    # Example pivot with stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`)\n",
    "    logger.info(\"Pivoting quota data.\")\n",
    "    pivoted_quota_df = quota_df.selectExpr(\n",
    "        \"Region\",\n",
    "        \"stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`) as (Year, Quota)\"\n",
    "    )\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 6: Calculated Fields\n",
    "    # Make sure the columns we reference are actually DateType, e.g. \"Order Date\", \"Ship Date\"\n",
    "    # This will only work if \"Order Date\" and \"Ship Date\" are valid date columns\n",
    "    logger.info(\"Adding calculated fields.\")\n",
    "    if \"Ship Date\" in all_orders_df.columns and \"Order Date\" in all_orders_df.columns:\n",
    "        all_orders_df = all_orders_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n",
    "\n",
    "    # Mark returned or not\n",
    "    # Now that we joined the return table, \"Return Reason\" should exist\n",
    "    all_orders_df = all_orders_df.withColumn(\"Returned?\", F.when(F.col(\"Return Reason\").isNotNull(), \"Yes\").otherwise(\"No\"))\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 7: Business Rules\n",
    "    # For example, filter out discount not in [17, 18]. \n",
    "    # Make sure discount is not null so the filter won't fail on nulls.\n",
    "    logger.info(\"Applying business rules.\")\n",
    "    all_orders_df = all_orders_df.filter(\n",
    "        (F.col(\"Discount\").isNotNull()) & \n",
    "        ((F.col(\"Discount\") < 17) | (F.col(\"Discount\") > 18))\n",
    "    )\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 8: Aggregation\n",
    "    # year(\"Order Date\") works only if it's DateType\n",
    "    logger.info(\"Aggregating data for annual regional performance.\")\n",
    "    aggregated_df = (\n",
    "        all_orders_df\n",
    "        .groupBy(\"Region\", F.year(F.col(\"Order Date\")).alias(\"Year of Sale\"))\n",
    "        .agg(\n",
    "            F.sum(\"Profit\").alias(\"Total Profit\"),\n",
    "            F.sum(\"Sales\").alias(\"Total Sales\"),\n",
    "            F.sum(\"Quantity\").alias(\"Total Quantity\"),\n",
    "            F.avg(\"Discount\").alias(\"Average Discount\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # COMMAND ----------\n",
    "    # Step 9: Output Generation\n",
    "    logger.info(\"Writing output to Unity Catalog tables.\")\n",
    "    #aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.citi.annual_regional_performance\")\n",
    "    aggregated_df.show()\n",
    "    #all_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.citi.superstore_sales\")\n",
    "    all_orders_df.show()\n",
    "    logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"An error occurred during the ETL process.\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
