{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC # Customer 360 Data Write\n# MAGIC \n# MAGIC This notebook writes the `comprehensive_df` DataFrame to a Delta table in the specified path.\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Import necessary libraries\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session (if not already initialized in the Databricks environment)\nspark = SparkSession.builder.appName(\"Customer360\").getOrCreate()\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Assuming comprehensive_df is already defined in the environment\n# If not, you would typically load or create this DataFrame here\n\n# Example: comprehensive_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/data.csv\")\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Write the DataFrame to a Delta table\ncomprehensive_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/output/Customer_360\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC \n# MAGIC The `comprehensive_df` DataFrame has been successfully written to the Delta table at `/mnt/output/Customer_360`.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}