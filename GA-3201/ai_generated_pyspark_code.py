"# Databricks notebook source\n# COMMAND ----------\n# %md\n# # ETL Process for Customer 360 Data\n# This notebook performs an ETL process to create a comprehensive Customer 360 view by ingesting data from various sources, transforming it, and writing the final output to a Unity Catalog table.\n\n# COMMAND ----------\n# %python\n# Import necessary libraries\nimport logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, max, avg, expr, current_date, datediff\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is pre-initialized as 'spark'\n\n# COMMAND ----------\n# %md\n# ## Step 1: Data Ingestion\n# Load data from Unity Catalog tables into DataFrames.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Loading data from Unity Catalog tables\")\n    policy_df = spark.table(\"catalog.source_db.policycsv\")\n    claims_df = spark.table(\"catalog.source_db.claimscsv\")\n    demographics_df = spark.table(\"catalog.source_db.demographicscsv\")\n    scores_df = spark.table(\"catalog.source_db.scorescsv\")\n    aiml_insights_df = spark.table(\"catalog.source_db.aiml_insightscsv\")\nexcept Exception as e:\n    logger.error(f\"Error loading data: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 2: Data Selection\n# Select relevant fields from each DataFrame.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Selecting relevant fields from each DataFrame\")\n    selected_demographics_df = demographics_df.select(\n        col(\"Customer_ID\"),\n        col(\"Customer_Name\"),\n        col(\"Email\"),\n        col(\"Phone_Number\"),\n        col(\"Address\"),\n        col(\"City\"),\n        col(\"State\"),\n        col(\"Postal_Code\"),\n        col(\"Date_of_Birth\"),\n        col(\"Gender\"),\n        col(\"Marital_Status\"),\n        col(\"Occupation\"),\n        col(\"Income_Level\"),\n        col(\"Customer_Segment\")\n    )\n    selected_claims_df = claims_df.select(\n        col(\"Claim_ID\"),\n        col(\"Policy_ID\"),\n        col(\"Claim_Date\"),\n        col(\"Claim_Type\"),\n        col(\"Claim_Status\"),\n        col(\"Claim_Amount\"),\n        col(\"Claim_Payout\")\n    )\n    selected_policy_df = policy_df.select(\n        col(\"policy_id\"),\n        col(\"customer_id\"),\n        col(\"policy_type\"),\n        col(\"policy_status\"),\n        col(\"policy_start_date\"),\n        col(\"policy_end_date\"),\n        col(\"policy_term\"),\n        col(\"policy_premium\"),\n        col(\"total_premium_paid\"),\n        col(\"renewal_status\"),\n        col(\"policy_addons\")\n    )\n    selected_aiml_insights_df = aiml_insights_df.select(\n        col(\"Customer_ID\"),\n        col(\"Churn_Probability\"),\n        col(\"Next_Best_Offer\"),\n        col(\"Claims_Fraud_Probability\"),\n        col(\"Revenue_Potential\")\n    )\n    selected_scores_df = scores_df.select(\n        col(\"Customer_ID\"),\n        col(\"Credit_Score\"),\n        col(\"Fraud_Score\"),\n        col(\"Customer_Risk_Score\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error selecting fields: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 3: Data Joining\n# Join datasets on common keys to create a unified DataFrame.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Joining datasets on common keys\")\n    joined_df = selected_demographics_df.join(\n        selected_policy_df, col('Customer_ID') == col('customer_id'), \"inner\"\n    ).join(\n        selected_claims_df, col('policy_id') == col('Policy_ID'), \"inner\"\n    )\nexcept Exception as e:\n    logger.error(f\"Error joining data: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 4: Data Aggregation\n# Aggregate data to calculate metrics such as total claims and average claim amount.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Aggregating data to calculate metrics\")\n    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n        count(\"Claim_ID\").alias(\"Total_Claims\"),\n        count(\"policy_id\").alias(\"Policy_Count\"),\n        max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error aggregating data: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 5: Additional Calculations\n# Perform additional calculations to derive new metrics.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Calculating additional metrics\")\n    claim_to_premium_ratio_expr = \"CASE WHEN total_premium_paid > 0 THEN Claim_Amount / total_premium_paid ELSE 0 END\"\n    claims_per_policy_expr = \"CASE WHEN Policy_Count > 0 THEN Total_Claims / Policy_Count ELSE 0 END\"\n    \n    final_df = aggregated_df.withColumn(\"Age\", expr(\"floor(datediff(current_date(), Date_of_Birth) / 365)\")) \\\n        .withColumn(\"Claim_To_Premium_Ratio\", expr(claim_to_premium_ratio_expr)) \\\n        .withColumn(\"Claims_Per_Policy\", expr(claims_per_policy_expr)) \\\n        .withColumn(\"Retention_Rate\", expr(\"0.85\")) \\\n        .withColumn(\"Cross_Sell_Opportunities\", expr(\"'Multi-Policy Discount, Home Coverage Addon'\")) \\\n        .withColumn(\"Upsell_Potential\", expr(\"'Premium Vehicle Coverage'\"))\nexcept Exception as e:\n    logger.error(f\"Error in additional calculations: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 6: Final Data Joining\n# Combine all processed data into a single DataFrame for the Customer 360 view.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Combining all processed data into a single DataFrame\")\n    customer_360_df = final_df.join(selected_aiml_insights_df, \"Customer_ID\", \"inner\") \\\n        .join(selected_scores_df, \"Customer_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error in final data joining: {e}\")\n    raise\n\n# COMMAND ----------\n# %md\n# ## Step 7: Data Output\n# Write the final DataFrame to a Unity Catalog table.\n\n# COMMAND ----------\n# %python\ntry:\n    logger.info(\"Writing the final DataFrame to Unity Catalog table\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.Customer_360csv\")\n    customer_360_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.Customer_360csv\")\nexcept Exception as e:\n    logger.error(f\"Error writing data: {e}\")\n    raise\n\nlogger.info(\"ETL process completed successfully\")\n"