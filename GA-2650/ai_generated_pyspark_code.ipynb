{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process for Superstore Sales Data\n# MAGIC This notebook performs an ETL process on Superstore sales data, including data loading, standardization, cleaning, integration, and aggregation.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\ntry:\n    # Step 1: Data Loading\n    logger.info(\"Loading data from Unity Catalog tables.\")\n    orders_central_df = spark.table(\"genai_demo.citi.orders_central\")\n    orders_east_df = spark.table(\"genai_demo.citi.orders_east\")\n    orders_south_2015_df = spark.table(\"genai_demo.citi.orders_south_2015\")\n    orders_south_2016_df = spark.table(\"genai_demo.citi.orders_south_2016\")\n    orders_south_2017_df = spark.table(\"genai_demo.citi.orders_south_2017\")\n    orders_south_2018_df = spark.table(\"genai_demo.citi.orders_south_2018\")\n    orders_west_df = spark.table(\"genai_demo.citi.orders_west\")\n    quota_df = spark.table(\"genai_demo.citi.quota\")\n    returns_df = spark.table(\"genai_demo.citi.returns\")\n\n# COMMAND ----------\n\n    # Step 2: Data Standardization\n    logger.info(\"Standardizing data formats and column names.\")\n    def standardize_dates(df, date_cols):\n        return df.withColumn(\n            \"Order Date\", F.to_date(F.concat(F.col(date_cols[0]), F.lit(\"/\"), F.col(date_cols[1]), F.lit(\"/\"), F.col(date_cols[2])), \"dd/MM/yyyy\")\n        ).drop(*date_cols)\n\n    # Apply date standardization to all datasets with separate date components\n    orders_central_df = standardize_dates(orders_central_df, [\"Order Day\", \"Order Month\", \"Order Year\"])\n    # Repeat for other datasets if needed\n\n# COMMAND ----------\n\n    # Step 3: Data Cleaning\n    logger.info(\"Cleaning data by filtering null Order IDs and filling missing values.\")\n    orders_central_df = orders_central_df.filter(F.col(\"Order ID\").isNotNull())\n    orders_central_df = orders_central_df.fillna({\"Discount\": 0})\n\n# COMMAND ----------\n\n    # Step 4: Data Integration and Consolidation\n    logger.info(\"Integrating and consolidating data from different regions.\")\n    all_orders_df = orders_central_df.unionByName(orders_east_df).unionByName(orders_south_2015_df).unionByName(orders_south_2016_df).unionByName(orders_south_2017_df).unionByName(orders_south_2018_df).unionByName(orders_west_df)\n\n    # Unpivot quota data\n    quota_unpivoted_df = quota_df.selectExpr(\"Region\", \"stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`) as (Year, Quota)\")\n\n# COMMAND ----------\n\n    # Step 5: Calculated Fields and Enhancements\n    logger.info(\"Adding calculated fields such as Days to Ship and Returned?\")\n    all_orders_df = all_orders_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n\n    # Join with returns to add Returned? column\n    all_orders_df = all_orders_df.join(returns_df, [\"Order ID\", \"Product ID\"], \"left\").withColumn(\"Returned?\", F.when(F.col(\"Return Reason\").isNotNull(), \"Yes\").otherwise(\"No\"))\n\n# COMMAND ----------\n\n    # Step 6: Business Rules Application\n    logger.info(\"Applying business rules to filter data.\")\n    all_orders_df = all_orders_df.filter(~((F.col(\"Discount\") >= 17) & (F.col(\"Discount\") <= 18)))\n\n# COMMAND ----------\n\n    # Step 7: Aggregation and Output\n    logger.info(\"Aggregating sales metrics by region and year.\")\n    all_orders_df = all_orders_df.withColumn(\"Year of Sale\", F.year(F.col(\"Order Date\")))\n    annual_performance_df = all_orders_df.groupBy(\"Region\", \"Year of Sale\").agg(\n        F.sum(\"Profit\").alias(\"Total Profit\"),\n        F.sum(\"Sales\").alias(\"Total Sales\"),\n        F.sum(\"Quantity\").alias(\"Total Quantity\"),\n        F.avg(\"Discount\").alias(\"Average Discount\")\n    )\n\n    # Write outputs to Unity Catalog tables\n    logger.info(\"Writing aggregated data to Unity Catalog tables.\")\n    annual_performance_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.citi.annual_regional_performance\")\n    all_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.citi.superstore_sales\")\n\n    logger.info(\"ETL process completed successfully.\")\n\n# COMMAND ----------\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process: %s\", e)\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}