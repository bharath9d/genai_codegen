{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4feff47-aaf9-444f-91b1-14b76e6a8cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Sales Data\n",
    "# MAGIC This notebook performs an ETL process on sales data using PySpark in Databricks.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def load_data_from_unity_catalog(table_name: str) -> DataFrame:\n",
    "    \"\"\"Load data from Unity Catalog table.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading data from Unity Catalog table: {table_name}\")\n",
    "        df = spark.table(table_name)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data from {table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def join_dataframes(df1: DataFrame, df2: DataFrame, join_column: str, join_type: str = \"inner\") -> DataFrame:\n",
    "    \"\"\"Join two DataFrames on a specified column, selecting only necessary columns.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Joining DataFrames on column: {join_column}\")\n",
    "        # Select necessary columns from each DataFrame\n",
    "        df1_selected = df1.select(join_column, *[col for col in df1.columns if col != join_column])\n",
    "        df2_selected = df2.select(join_column, *[col for col in df2.columns if col != join_column])\n",
    "        \n",
    "        joined_df = df1_selected.join(df2_selected, join_column, join_type)\n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error joining DataFrames on {join_column}: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def calculate_compensation(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Calculate total compensation for each associate.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Calculating total compensation for each associate\")\n",
    "        df = df.withColumn(\n",
    "            \"Compensation\",\n",
    "            F.col(\"Base_Salary\") + (F.col(\"Commission_Percentage\") / 100 * F.col(\"Base_Salary\")) + F.col(\"Bonus\")\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating compensation: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def calculate_projected_revenue(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Calculate projected revenue based on target year and growth rate.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Calculating projected revenue\")\n",
    "        df = df.withColumn(\n",
    "            \"Projected_Revenue\",\n",
    "            F.when(F.col(\"Target Year\") == 2024, F.col(\"Sales_Revenue\") * (F.col(\"Projected_Sales_Growth_Rate\") / 100))\n",
    "            .when(F.col(\"Target Year\") == 2025, F.col(\"Sales_Revenue\") * (1 + F.col(\"Projected_Sales_Growth_Rate\") / 100))\n",
    "            .when(F.col(\"Target Year\") == 2026, F.col(\"Sales_Revenue\") * (1 + F.col(\"Projected_Sales_Growth_Rate\") / 100))\n",
    "            .otherwise(F.col(\"Sales_Revenue\"))\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating projected revenue: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def calculate_projected_sales_growth_rate(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Adjust the projected sales growth rate based on the target year.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Calculating projected sales growth rate\")\n",
    "        df = df.withColumn(\n",
    "            \"Projected_Sales_Growth_Rate\",\n",
    "            F.when(F.col(\"Target Year\") == 2024, F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100))\n",
    "            .when(F.col(\"Target Year\") == 2025, F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100) * 2)\n",
    "            .when(F.col(\"Target Year\") == 2026, F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100) * 3)\n",
    "            .otherwise(F.col(\"Projected_Growth_Rate\"))\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating projected sales growth rate: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC\n",
    "def main():\n",
    "    try:\n",
    "        # Load data from Unity Catalog tables\n",
    "        hospital_stats_df = load_data_from_unity_catalog(\"genai_demo.cardinal_health.hospital_stats_north_america\")\n",
    "        sales_assignments_df = load_data_from_unity_catalog(\"genai_demo.cardinal_health.hospital_sales_assignments\")\n",
    "        employment_details_df = load_data_from_unity_catalog(\"genai_demo.cardinal_health.sales_associates_employment_details\")\n",
    "        compensation_guidelines_df = load_data_from_unity_catalog(\"genai_demo.cardinal_health.compensation_guidelines\")\n",
    "        historical_sales_df = load_data_from_unity_catalog(\"genai_demo.cardinal_health.historical_sales\")\n",
    "\n",
    "        # Join operations\n",
    "        hospital_sales_df = join_dataframes(hospital_stats_df, sales_assignments_df, \"Hospital_ID\")\n",
    "        employment_compensation_df = join_dataframes(employment_details_df, compensation_guidelines_df, \"Associate_ID\")\n",
    "        combined_df = join_dataframes(employment_compensation_df, hospital_sales_df, \"Associate_ID\")\n",
    "\n",
    "        # Calculate compensation\n",
    "        compensation_df = calculate_compensation(combined_df)\n",
    "\n",
    "        # Calculate projected revenue and sales growth rate\n",
    "        revenue_df = calculate_projected_revenue(historical_sales_df)\n",
    "        growth_rate_df = calculate_projected_sales_growth_rate(revenue_df)\n",
    "\n",
    "        # Write the final DataFrame to Unity Catalog table\n",
    "        logger.info(\"Writing the final DataFrame to Unity Catalog table\")\n",
    "        growth_rate_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.cardinal_health.Target_sales\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ETL process: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
