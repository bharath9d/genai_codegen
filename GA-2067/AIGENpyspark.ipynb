{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DateType\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import broadcast\n\n# COMMAND ----------\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\ndef load_data():\n    try:\n        # Load data from Unity Catalog tables\n        orders_central_df = spark.table(\"catalog.sales.orders_central\")\n        orders_west_df = spark.table(\"catalog.sales.orders_west\")\n        orders_east_df = spark.table(\"catalog.sales.orders_east\")\n        orders_south_df = spark.table(\"catalog.sales.orders_south\")\n        quota_df = spark.table(\"catalog.sales.quota\")\n        returns_df = spark.table(\"catalog.sales.returns\")\n        return orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df\n    except Exception as e:\n        logger.error(\"Error loading data\", exc_info=True)\n        raise\n\n# COMMAND ----------\ndef standardize_dates(df, date_columns):\n    for date_col in date_columns:\n        df = df.withColumn(date_col, F.to_date(F.col(date_col), \"dd/MM/yyyy\"))\n    return df\n\n# COMMAND ----------\ndef transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df):\n    try:\n        # Standardize Dates\n        logger.info(\"Standardizing and cleaning data...\")\n        orders_central_df = orders_central_df.withColumn(\"Order Date\", F.concat(F.col(\"Order Day\"), F.lit(\"/\"), F.col(\"Order Month\"), F.lit(\"/\"), F.col(\"Order Year\")))\n        orders_central_df = orders_central_df.withColumn(\"Ship Date\", F.concat(F.col(\"Ship Day\"), F.lit(\"/\"), F.col(\"Ship Month\"), F.lit(\"/\"), F.col(\"Ship Year\")))\n\n        # Convert date strings to date type\n        orders_central_df = standardize_dates(orders_central_df, [\"Order Date\", \"Ship Date\"])\n\n        # Union Operations\n        logger.info(\"Combining regional order datasets...\")\n        all_orders_df = orders_central_df.unionByName(orders_west_df).unionByName(orders_east_df).unionByName(orders_south_df)\n\n        # Join Operations\n        logger.info(\"Joining orders with returns data...\")\n        returns_df = returns_df.drop(\"Order Date\", \"Product ID\")\n        orders_returns_df = all_orders_df.join(broadcast(returns_df), [\"Order ID\"], \"left\")\n\n        # Custom Calculations\n        logger.info(\"Calculating custom fields...\")\n        orders_returns_df = orders_returns_df.withColumn(\"Days to Ship\", F.datediff(F.col(\"Ship Date\"), F.col(\"Order Date\")))\n        orders_returns_df = orders_returns_df.withColumn(\"Returned?\", F.when(F.col(\"Return Reason\").isNotNull(), \"Yes\").otherwise(\"No\"))\n        orders_returns_df = orders_returns_df.withColumn(\"Year of Sale\", F.year(F.col(\"Order Date\")))\n\n        return orders_returns_df\n    except Exception as e:\n        logger.error(\"Error during data transformation\", exc_info=True)\n        raise\n\n# COMMAND ----------\ndef aggregate_data(orders_returns_df):\n    try:\n        # Data Aggregation\n        logger.info(\"Aggregating sales metrics...\")\n        aggregated_df = orders_returns_df.groupBy(\"Region\", \"Year of Sale\").agg(\n            F.sum(\"Sales\").alias(\"Total Sales\"),\n            F.sum(\"Profit\").alias(\"Total Profit\"),\n            F.sum(\"Quantity\").alias(\"Total Quantity\"),\n            F.avg(\"Discount\").alias(\"Average Discount\")\n        )\n        return aggregated_df\n    except Exception as e:\n        logger.error(\"Error during data aggregation\", exc_info=True)\n        raise\n\n# COMMAND ----------\ndef write_data(aggregated_df, orders_returns_df):\n    try:\n        # Output to Unity Catalog\n        logger.info(\"Writing data to Unity Catalog tables...\")\n        aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.analytics.annual_regional_performance\")\n        orders_returns_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.analytics.superstore_sales\")\n    except Exception as e:\n        logger.error(\"Error writing data to Unity Catalog\", exc_info=True)\n        raise\n\n# COMMAND ----------\ndef main():\n    try:\n        orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df = load_data()\n        orders_returns_df = transform_data(orders_central_df, orders_west_df, orders_east_df, orders_south_df, returns_df)\n        aggregated_df = aggregate_data(orders_returns_df)\n        write_data(aggregated_df, orders_returns_df)\n        logger.info(\"ETL process completed successfully.\")\n    except Exception as e:\n        logger.error(\"An error occurred during the ETL process\", exc_info=True)\n\n# COMMAND ----------\nif __name__ == \"__main__\":\n    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}