{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# MAGIC %md\n# MAGIC # ETL Process with PySpark\n# MAGIC This notebook demonstrates an ETL process using PySpark, including data loading, standardization, cleaning, integration, and application of business rules.\n\n# COMMAND ----------\n\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.utils import AnalysisException\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Load Data from Unity Catalog\n# MAGIC Function to load data from Unity Catalog tables.\n\n# COMMAND ----------\n\ndef load_data_from_catalog(table_name):\n    try:\n        logger.info(f\"Loading data from Unity Catalog table: {table_name}\")\n        df = spark.table(table_name)\n        logger.info(f\"Loaded data schema: {df.schema}\")\n        return df\n    except AnalysisException as e:\n        logger.error(f\"Error loading data from table {table_name}: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Standardize Data\n# MAGIC Function to standardize data formats.\n\n# COMMAND ----------\n\ndef standardize_data(df):\n    try:\n        logger.info(\"Standardizing data formats\")\n        df_standardized = df.withColumn(\"sale_date\", F.to_date(F.col(\"sale_date\"), \"MM/dd/yyyy\"))\n        logger.info(f\"Standardized data schema: {df_standardized.schema}\")\n        return df_standardized\n    except Exception as e:\n        logger.error(f\"Error during data standardization: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Clean Data\n# MAGIC Function to clean data by removing duplicates and handling null values.\n\n# COMMAND ----------\n\ndef clean_data(df):\n    try:\n        logger.info(\"Cleaning data\")\n        df_cleaned = df.dropDuplicates()\n        df_cleaned = df_cleaned.fillna({\"sales_amount\": 0})\n        df_cleaned = df_cleaned.withColumn(\"sales_amount\", F.when(F.col(\"sales_amount\") < 0, 0).otherwise(F.col(\"sales_amount\")))\n        logger.info(f\"Cleaned data schema: {df_cleaned.schema}\")\n        return df_cleaned\n    except Exception as e:\n        logger.error(f\"Error during data cleaning: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Integrate Data\n# MAGIC Function to integrate data from multiple sources.\n\n# COMMAND ----------\n\ndef integrate_data(df_list):\n    try:\n        logger.info(\"Integrating data from multiple sources\")\n        df_integrated = df_list[0]\n        for df in df_list[1:]:\n            df_integrated = df_integrated.unionByName(df)\n        logger.info(f\"Integrated data schema: {df_integrated.schema}\")\n        return df_integrated\n    except Exception as e:\n        logger.error(f\"Error during data integration: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Apply Business Rules\n# MAGIC Function to apply business rules to the data.\n\n# COMMAND ----------\n\ndef apply_business_rules(df):\n    try:\n        logger.info(\"Applying business rules\")\n        df_final = df.withColumn(\"total_sales\", F.col(\"quantity\") * F.col(\"price\"))\n        logger.info(f\"Final data schema: {df_final.schema}\")\n        return df_final\n    except Exception as e:\n        logger.error(f\"Error applying business rules: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Write Data to Unity Catalog\n# MAGIC Function to write the final DataFrame to Unity Catalog.\n\n# COMMAND ----------\n\ndef write_data_to_catalog(df, target_table):\n    try:\n        logger.info(f\"Writing data to Unity Catalog table: {target_table}\")\n        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n    except Exception as e:\n        logger.error(f\"Error writing data to table {target_table}: {e}\")\n        raise\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## Main ETL Process\n# MAGIC Execute the main ETL process, which includes loading, standardizing, cleaning, integrating, applying business rules, and writing data.\n\n# COMMAND ----------\n\ndef main():\n    try:\n        # Load data from Unity Catalog tables\n        df_region1 = load_data_from_catalog(\"catalog.source_db.region1_sales\")\n        df_region2 = load_data_from_catalog(\"catalog.source_db.region2_sales\")\n\n        # Standardize data\n        df_region1_standardized = standardize_data(df_region1)\n        df_region2_standardized = standardize_data(df_region2)\n\n        # Clean data\n        df_region1_cleaned = clean_data(df_region1_standardized)\n        df_region2_cleaned = clean_data(df_region2_standardized)\n\n        # Integrate data\n        df_integrated = integrate_data([df_region1_cleaned, df_region2_cleaned])\n\n        # Apply business rules\n        df_final = apply_business_rules(df_integrated)\n\n        # Write the final DataFrame to Unity Catalog\n        write_data_to_catalog(df_final, \"catalog.target_db.sales_data_transformed\")\n\n    except Exception as e:\n        logger.error(f\"ETL process failed: {e}\")\n\n# Execute the main ETL process\nif __name__ == \"__main__\":\n    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}