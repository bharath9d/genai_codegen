{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3c69e7-56cf-46f1-ae11-6b6880beedcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # ETL Process for Cardinal Health Data\n",
    "# MAGIC This notebook performs an ETL process on data from Unity Catalog tables, integrating and transforming the data to produce a final output table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    # Load data from Unity Catalog tables\n",
    "    logger.info(\"Loading data from Unity Catalog tables...\")\n",
    "    df_employment = spark.table(\"genai_demo.cardinal_health.sales_associates_employment_details\").select(\"Associate_ID\", \"Associate_Name\", \"Region\", \"Employment_Type\", \"Years_of_Experience\")\n",
    "    df_goals = spark.table(\"genai_demo.cardinal_health.Company_Goals\").select(\"Hospital_ID\", \"Channel_Type\", \"Growth_Target\", \"Investment_Planned\")\n",
    "    df_compensation = spark.table(\"genai_demo.cardinal_health.Compensation_Guidelines\").select(\"Associate_ID\", \"Base_Salary\", \"Commission_Percentage\", \"Bonus\")\n",
    "    df_growth = spark.table(\"genai_demo.cardinal_health.Growth_Opportunities\").select(\"Channel_ID\", \"Channel_Type\", \"Market_Potential\", \"Projected_Growth_Rate\", \"Investment_Required\", \"Expected_ROI\")\n",
    "    df_sales = spark.table(\"genai_demo.cardinal_health.Historical_Sales\").select(\"Hospital_ID\", \"Channel_Type\", \"Sales_Revenue\", \"Year\")\n",
    "    df_assignments = spark.table(\"genai_demo.cardinal_health.hospital_sales_assignments\").select(\"Associate_ID\", \"Hospital_ID\")\n",
    "    df_hospital_stats = spark.table(\"genai_demo.cardinal_health.hospital_stats_north_america\").select(\"Hospital_ID\", \"Hospital_Name\", \"City\", \"State\", \"Number_of_Beds\", \"Annual_Revenue\", \"Patient_Satisfaction_Score\")\n",
    "    df_logistics = spark.table(\"genai_demo.cardinal_health.Logistics_Channels\").select(\"Channel_ID\", \"Channel_Type\", \"Hospital_ID\", \"Growth_Opportunities\")\n",
    "    df_trends = spark.table(\"genai_demo.cardinal_health.third_party_sales_trends\").select(\"Channel_Type\", \"Market_Trend\", \"Political_Impact\", \"Economic_Impact\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Data Integration: Join tables based on common identifiers\n",
    "    logger.info(\"Joining data tables...\")\n",
    "    df_joined = df_employment.join(broadcast(df_compensation), \"Associate_ID\", \"inner\") \\\n",
    "                             .join(df_assignments, \"Associate_ID\", \"inner\") \\\n",
    "                             .join(df_hospital_stats, \"Hospital_ID\", \"inner\") \\\n",
    "                             .join(df_logistics, \"Hospital_ID\", \"inner\") \\\n",
    "                             .join(df_growth, [\"Channel_ID\", \"Channel_Type\"], \"inner\") \\\n",
    "                             .join(df_sales, [\"Hospital_ID\", \"Channel_Type\"], \"inner\") \\\n",
    "                             .join(df_goals, [\"Hospital_ID\", \"Channel_Type\"], \"inner\") \\\n",
    "                             .join(df_trends, \"Channel_Type\", \"inner\")\n",
    "\n",
    "    # Cache the joined DataFrame if used multiple times\n",
    "    df_joined.cache()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Custom Calculations: Calculate total compensation\n",
    "    logger.info(\"Calculating total compensation...\")\n",
    "    df_joined = df_joined.withColumn(\"Compensation\", \n",
    "                                     F.col(\"Base_Salary\") + \n",
    "                                     (F.col(\"Commission_Percentage\") * F.col(\"Base_Salary\")) + \n",
    "                                     F.col(\"Bonus\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Custom Calculations: Calculate projected revenue\n",
    "    logger.info(\"Calculating projected revenue...\")\n",
    "    df_joined = df_joined.withColumn(\"Projected_Revenue\", \n",
    "                                     F.when(F.col(\"Year\") == 2024, F.col(\"Sales_Revenue\") * (F.col(\"Projected_Growth_Rate\") / 100))\n",
    "                                      .when(F.col(\"Year\") == 2025, F.col(\"Sales_Revenue\") * (1 + F.col(\"Projected_Growth_Rate\") / 100))\n",
    "                                      .when(F.col(\"Year\") == 2026, F.col(\"Sales_Revenue\") * (1 + F.col(\"Projected_Growth_Rate\") / 100))\n",
    "                                      .otherwise(F.col(\"Sales_Revenue\")))\n",
    "    df_joined.show()\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Data Filtering and Sorting\n",
    "    logger.info(\"Filtering and sorting data...\")\n",
    "    df_filtered = df_joined.filter(F.col(\"Year\") > 2023).orderBy(\"Year\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Data Preparation for Output: Select relevant fields\n",
    "    logger.info(\"Selecting relevant fields for output...\")\n",
    "    df_output = df_filtered.select(\"Hospital_ID\", \"Compensation\", \"Projected_Revenue\", \"Year\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "    # Output Data: Write the final processed data to Unity Catalog\n",
    "    logger.info(\"Writing output data to Unity Catalog...\")\n",
    "    # df_output.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.cardinal_health.Hospitals_Output\")\n",
    "    df_output.show()\n",
    "    logger.info(\"ETL process completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during the ETL process: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_generated_pyspark_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
